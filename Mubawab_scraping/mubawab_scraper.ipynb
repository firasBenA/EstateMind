{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176f2cca-3751-4b4a-b760-f253420199d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration chargÃ©e.\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, uuid, json, random, requests, psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# â”€â”€ Base de donnÃ©es â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DB_CONFIG = {\n",
    "    \"host\":     \"localhost\",\n",
    "    \"database\": \"estate_mind_db\",\n",
    "    \"user\":     \"postgres\",\n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "# â”€â”€ Dossier mÃ©dia â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MEDIA_ROOT = \"media\"\n",
    "os.makedirs(MEDIA_ROOT, exist_ok=True)\n",
    "\n",
    "# â”€â”€ ParamÃ¨tres de scraping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TARGET_PER_CATEGORY    = 300   # 300 vente + 300 location = 600 par type\n",
    "MAX_PAGES_PER_CATEGORY = 15    # ~25 annonces/page â†’ 15 pages suffisent\n",
    "\n",
    "# â”€â”€ CatÃ©gories (slug â†’ transaction_type, property_type) â”€â”€â”€â”€â”€\n",
    "CATEGORIES = {\n",
    "    \"sc/appartements-a-vendre\":          (\"sale\", \"apartment\"),\n",
    "    \"sc/appartements-a-louer\":           (\"rent\", \"apartment\"),\n",
    "    \"sc/villas-a-vendre\":                (\"sale\", \"villa\"),\n",
    "    \"sc/villas-a-louer\":                 (\"rent\", \"villa\"),\n",
    "    \"sc/terrains-a-vendre\":              (\"sale\", \"land\"),\n",
    "    \"sc/terrains-a-louer\":              (\"rent\", \"land\"),\n",
    "    \"sc/bureaux-et-plateaux-a-vendre\":   (\"sale\", \"office\"),\n",
    "    \"sc/bureaux-et-plateaux-a-louer\":    (\"rent\", \"office\"),\n",
    "    \"sc/maisons-a-vendre\":               (\"sale\", \"house\"),\n",
    "    \"sc/maisons-a-louer\":                (\"rent\", \"house\"),\n",
    "}\n",
    "\n",
    "# â”€â”€ Mapping features FR â†’ EN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FEATURE_MAPPING = {\n",
    "    \"Climatisation\":     \"has_air_conditioning\",\n",
    "    \"Chauffage central\": \"has_central_heating\",\n",
    "    \"Ascenseur\":         \"has_elevator\",\n",
    "    \"Jardin\":            \"has_garden\",\n",
    "    \"Piscine\":           \"has_pool\",\n",
    "    \"Terrasse\":          \"has_terrace\",\n",
    "    \"Garage\":            \"has_garage\",\n",
    "    \"Parking\":           \"has_parking\",\n",
    "    \"Vue sur mer\":       \"has_sea_view\",\n",
    "    \"MeublÃ©\":            \"is_furnished\",\n",
    "    \"Cuisine Ã©quipÃ©e\":   \"has_equipped_kitchen\",\n",
    "    \"Interphone\":        \"has_intercom\",\n",
    "    \"Balcon\":            \"has_balcony\",\n",
    "    \"Cave\":              \"has_cellar\",\n",
    "    \"SÃ©curitÃ©\":          \"has_security\",\n",
    "    \"Digicode\":          \"has_digicode\",\n",
    "    \"VidÃ©osurveillance\": \"has_cctv\",\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration chargÃ©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dc5edf-6b76-441a-bb01-b9c3985ecf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table 'mubawab_listings' crÃ©Ã©e avec succÃ¨s.\n",
      "âœ… Table crÃ©Ã©e + toutes les fonctions prÃªtes.\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. CRÃ‰ATION TABLE (depuis zÃ©ro)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cur  = conn.cursor()\n",
    "\n",
    "cur.execute(\"DROP TABLE IF EXISTS mubawab_listings;\")\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE mubawab_listings (\n",
    "        id               SERIAL PRIMARY KEY,\n",
    "\n",
    "        -- Infos principales\n",
    "        title            TEXT,\n",
    "        subtitle         TEXT,\n",
    "        price            VARCHAR(100),\n",
    "        transaction_type VARCHAR(20),    -- 'sale' | 'rent'\n",
    "        property_type    VARCHAR(30),    -- 'apartment' | 'villa' | 'land' | 'office' | 'house'\n",
    "\n",
    "        -- GÃ©ographie structurÃ©e\n",
    "        location_details JSONB,          -- {city, municipality, district, zone, raw}\n",
    "        municipality     VARCHAR(100),   -- extrait de location_details pour requÃªtes rapides\n",
    "\n",
    "        -- CaractÃ©ristiques numÃ©riques (extraites intelligemment du titre/description)\n",
    "        surface          FLOAT,          -- superficie en mÂ² (ex: 132.0)\n",
    "        rooms            INTEGER,        -- nombre de piÃ¨ces (ex: 2 pour S2)\n",
    "        bedrooms         VARCHAR(30),    -- brut depuis la page dÃ©tail\n",
    "        bathrooms        VARCHAR(30),    -- brut depuis la page dÃ©tail\n",
    "\n",
    "        -- DonnÃ©es riches\n",
    "        features         JSONB,          -- {has_elevator: true, has_garden: true, ...}\n",
    "        poi              JSONB,          -- points d'intÃ©rÃªt proches\n",
    "\n",
    "        -- Texte\n",
    "        description      TEXT,\n",
    "\n",
    "        -- Source\n",
    "        url              TEXT UNIQUE,\n",
    "\n",
    "        -- Images\n",
    "        image_path       TEXT,           -- chemin miniature principale\n",
    "        images_folder    TEXT,           -- dossier contenant toutes les images\n",
    "        images_count     INTEGER DEFAULT 0,\n",
    "\n",
    "        -- MÃ©tadonnÃ©es\n",
    "        scraped_at       TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        last_updated     VARCHAR(100)    -- date affichÃ©e sur l'annonce\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"âœ… Table 'mubawab_listings' crÃ©Ã©e avec succÃ¨s.\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. FONCTIONS UTILITAIRES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def get_geo_zone(city: str) -> str:\n",
    "    c = city.lower()\n",
    "    if any(x in c for x in [\"tunis\",\"ariana\",\"ben arous\",\"manouba\",\"marsa\",\n",
    "                              \"carthage\",\"la goulette\",\"bardo\",\"ezzahra\"]):\n",
    "        return \"grand_tunis\"\n",
    "    if any(x in c for x in [\"sousse\",\"monastir\",\"mahdia\"]):\n",
    "        return \"sahel\"\n",
    "    if any(x in c for x in [\"sfax\",\"gabes\",\"jerba\",\"djerba\",\"medenine\",\n",
    "                              \"tataouine\",\"tozeur\",\"kebili\",\"zarzis\"]):\n",
    "        return \"sud\"\n",
    "    if any(x in c for x in [\"bizerte\",\"nabeul\",\"hammamet\",\"beja\",\n",
    "                              \"jendouba\",\"zaghouan\",\"kelibia\",\"korba\"]):\n",
    "        return \"nord\"\n",
    "    if any(x in c for x in [\"kairouan\",\"kasserine\",\"sidi bouzid\",\"siliana\",\"gafsa\"]):\n",
    "        return \"centre\"\n",
    "    return \"autre\"\n",
    "\n",
    "def build_location(subtitle: str) -> tuple[dict, str]:\n",
    "    \"\"\"Retourne (dict JSON, municipality string)\"\"\"\n",
    "    parts = [p.strip() for p in subtitle.split(',')]\n",
    "    city         = parts[-1] if parts else \"Tunisie\"\n",
    "    municipality = parts[-2] if len(parts) >= 2 else city\n",
    "    district     = parts[0]  if len(parts) >= 3 else \"\"\n",
    "    loc = {\n",
    "        \"city\":         city,\n",
    "        \"municipality\": municipality,\n",
    "        \"district\":     district,\n",
    "        \"zone\":         get_geo_zone(city),\n",
    "        \"raw\":          subtitle\n",
    "    }\n",
    "    return loc, municipality\n",
    "\n",
    "def extract_rooms(text: str) -> int | None:\n",
    "    \"\"\"S2, s+2, S 2, F3, T4, studio, 2 piÃ¨ces, 2 chambres â†’ entier\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.lower()\n",
    "    for pat in [\n",
    "        r'\\bs\\s*\\+?\\s*(\\d)\\b',        # S2, S+2, s 2\n",
    "        r'\\b[ft]\\s*(\\d)\\b',            # F3, T4\n",
    "        r'(\\d)\\s*pi[eÃ¨]ces?',          # 3 piÃ¨ces\n",
    "        r'(\\d)\\s*chambres?',           # 2 chambres\n",
    "        r'\\bstudio\\b',                 # studio = 1\n",
    "    ]:\n",
    "        m = re.search(pat, t)\n",
    "        if m:\n",
    "            return 1 if 'studio' in pat else (int(m.group(1)) if 1 <= int(m.group(1)) <= 10 else None)\n",
    "    return None\n",
    "\n",
    "def extract_surface(text: str) -> float | None:\n",
    "    \"\"\"45mÂ², 132 m2, superficie de 85 â†’ float\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.lower()\n",
    "    for pat in [\n",
    "        r'(\\d[\\d\\s.,]*)\\s*m\\s*[Â²2]',\n",
    "        r'(?:superficie|surface|superfici)\\s*(?:de|:)?\\s*(\\d[\\d.,]*)',\n",
    "        r'de\\s+(\\d[\\d.,]*)\\s*m\\s*[Â²2]',\n",
    "    ]:\n",
    "        m = re.search(pat, t)\n",
    "        if m:\n",
    "            try:\n",
    "                val = float(m.group(1).replace(' ','').replace(',','.'))\n",
    "                if 10 <= val <= 10000:\n",
    "                    return val\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def smart_extract(title: str, description: str) -> tuple[int | None, float | None]:\n",
    "    \"\"\"Retourne (rooms, surface) en cherchant dans titre puis description.\"\"\"\n",
    "    rooms   = extract_rooms(title)   or extract_rooms(description)\n",
    "    surface = extract_surface(title) or extract_surface(description)\n",
    "    return rooms, surface\n",
    "\n",
    "def map_features(raw_list: list) -> dict:\n",
    "    result = {}\n",
    "    for item in raw_list:\n",
    "        item = item.strip()\n",
    "        if not item:\n",
    "            continue\n",
    "        mapped = False\n",
    "        for fr_key, en_key in FEATURE_MAPPING.items():\n",
    "            if fr_key.lower() in item.lower():\n",
    "                result[en_key] = True\n",
    "                mapped = True\n",
    "                break\n",
    "        if not mapped and len(item) < 60:\n",
    "            result[item] = True\n",
    "    return result\n",
    "\n",
    "def download_images(img_urls: list, prop_type: str, listing_id: int) -> tuple[str | None, str, int]:\n",
    "    \"\"\"\n",
    "    TÃ©lÃ©charge toutes les images dans media/{prop_type}/{listing_id}/\n",
    "    Retourne (chemin_miniature, chemin_dossier, nb_images)\n",
    "    \"\"\"\n",
    "    if not img_urls:\n",
    "        folder = os.path.join(MEDIA_ROOT, prop_type, str(listing_id))\n",
    "        return None, folder, 0\n",
    "\n",
    "    folder = os.path.join(MEDIA_ROOT, prop_type, str(listing_id))\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    saved = []\n",
    "    for idx, url in enumerate(img_urls, start=1):\n",
    "        if not url or url.startswith(\"data:\"):\n",
    "            continue\n",
    "        try:\n",
    "            r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=8)\n",
    "            if r.status_code == 200:\n",
    "                ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                ext = ext if ext in [\"jpg\",\"jpeg\",\"png\",\"webp\"] else \"jpg\"\n",
    "                filepath = os.path.join(folder, f\"image_{idx:03d}.{ext}\")\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                saved.append(filepath)\n",
    "                time.sleep(0.15)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    thumb = saved[0] if saved else None\n",
    "    return thumb, folder, len(saved)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. SELENIUM\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def create_driver():\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    opts.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()), options=opts\n",
    "    )\n",
    "    driver.execute_script(\n",
    "        \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. SCRAPING PAGE DÃ‰TAIL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def scrape_detail(driver, url: str) -> dict:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2.5, 4))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "        time.sleep(0.8)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        data = {}\n",
    "\n",
    "        # Description\n",
    "        for sel in [\".blockProp p\", \".description p\", \"#adDescription\", \".adDesc\"]:\n",
    "            el = soup.select_one(sel)\n",
    "            if el:\n",
    "                data[\"description\"] = el.get_text(\" \", strip=True)\n",
    "                break\n",
    "        data.setdefault(\"description\", \"\")\n",
    "\n",
    "        # Date\n",
    "        for sel in [\".creationDate\", \"#creationDate\", \".adDate\", \".listingDate\"]:\n",
    "            el = soup.select_one(sel)\n",
    "            if el:\n",
    "                data[\"last_updated\"] = el.get_text(strip=True)\n",
    "                break\n",
    "        data.setdefault(\"last_updated\", \"Unknown\")\n",
    "\n",
    "        # Chambres / salles de bain (brut, page dÃ©tail)\n",
    "        for feat in soup.select(\".adMainFeature, .adFeature, .listingFeature\"):\n",
    "            txt = feat.get_text(strip=True)\n",
    "            if \"Chambre\" in txt:\n",
    "                data[\"bedrooms\"] = txt.replace(\"Chambres\",\"\").replace(\"Chambre\",\"\").strip()\n",
    "            elif \"Bain\" in txt or \"Salle\" in txt:\n",
    "                data[\"bathrooms\"] = txt.replace(\"Salles de bains\",\"\").replace(\"Salle de bain\",\"\").strip()\n",
    "\n",
    "        # Features\n",
    "        raw_feats = [li.get_text(strip=True) for li in soup.select(\n",
    "            \".adDetailsList li, .features li, .equipements li, ul.featureList li\"\n",
    "        )]\n",
    "        data[\"features\"] = map_features(raw_feats)\n",
    "\n",
    "        # POI\n",
    "        poi = {}\n",
    "        for p in soup.select(\".poiList li, .poi li, .nearbyList li\"):\n",
    "            txt = p.get_text(strip=True)\n",
    "            if txt:\n",
    "                poi[txt] = \"detected\"\n",
    "        data[\"poi\"] = poi\n",
    "\n",
    "        # Images (toutes)\n",
    "        img_urls = []\n",
    "        seen     = set()\n",
    "        for sel in [\".sliderPhoto img\", \".gallery img\", \".photoGallery img\",\n",
    "                    \"#listingGallery img\", \".adGallery img\", \"img.bigPhoto\"]:\n",
    "            for img in soup.select(sel):\n",
    "                src = img.get(\"data-big\") or img.get(\"data-src\") or img.get(\"src\",\"\")\n",
    "                if src and \"http\" in src and src not in seen:\n",
    "                    img_urls.append(re.sub(r'_thumb|_small|_medium','', src))\n",
    "                    seen.add(src)\n",
    "\n",
    "        # Fallback\n",
    "        if not img_urls:\n",
    "            for img in soup.select(\"img\"):\n",
    "                src = img.get(\"data-src\") or img.get(\"src\",\"\")\n",
    "                if (src and \"http\" in src and src not in seen\n",
    "                        and any(e in src.lower() for e in [\".jpg\",\".jpeg\",\".png\",\".webp\"])\n",
    "                        and \"logo\" not in src.lower() and \"icon\" not in src.lower()):\n",
    "                    img_urls.append(src)\n",
    "                    seen.add(src)\n",
    "\n",
    "        data[\"img_urls\"] = img_urls\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸ DÃ©tail error: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. SCRAPING PAGE LISTE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def scrape_list_page(driver, url: str):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(3, 5))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for sel in [\"li.listingBox\", \"li.promotionListing\", \".ulListing li\",\n",
    "                \"div.listingBox\", \"article.listing\", \"[data-url]\"]:\n",
    "        found = soup.select(sel)\n",
    "        if found:\n",
    "            return found, sel\n",
    "    return [], None\n",
    "\n",
    "def extract_basic(item, trans_type, prop_type):\n",
    "    try:\n",
    "        link = (\n",
    "            item.select_one(\"h2.listingTit a\") or\n",
    "            item.select_one(\"h2 a\") or\n",
    "            item.select_one(\"a[href*='/fr/']\")\n",
    "        )\n",
    "        if not link:\n",
    "            return None\n",
    "\n",
    "        title = link.get_text(strip=True)\n",
    "        url   = link.get(\"href\",\"\")\n",
    "        if url and not url.startswith(\"http\"):\n",
    "            url = \"https://www.mubawab.tn\" + url\n",
    "        if not url:\n",
    "            return None\n",
    "\n",
    "        price_el = item.select_one(\"span.priceTag, .price, [class*='price']\")\n",
    "        price = price_el.get_text(strip=True).replace(\"\\xa0\",\"\").replace(\"\\u202f\",\"\") \\\n",
    "                if price_el else \"N/A\"\n",
    "\n",
    "        sub_el   = item.select_one(\"span.listingDetails, .adLocation, .location\")\n",
    "        subtitle = sub_el.get_text(strip=True) if sub_el else \"Tunisie\"\n",
    "\n",
    "        img_el   = (item.select_one(\"img.photo-listing\") or\n",
    "                    item.select_one(\"img.listingImg\") or\n",
    "                    item.select_one(\".imgContainer img\") or\n",
    "                    item.select_one(\"img\"))\n",
    "        thumb = None\n",
    "        if img_el:\n",
    "            thumb = img_el.get(\"data-src\") or img_el.get(\"data-lazy\") or img_el.get(\"src\")\n",
    "            if thumb and thumb.startswith(\"//\"):\n",
    "                thumb = \"https:\" + thumb\n",
    "\n",
    "        return {\n",
    "            \"title\": title, \"subtitle\": subtitle, \"price\": price,\n",
    "            \"transaction_type\": trans_type, \"property_type\": prop_type,\n",
    "            \"thumb\": thumb, \"url\": url,\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ… Table crÃ©Ã©e + toutes les fonctions prÃªtes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fa406-cd7a-4c29-be3e-eedea5022400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ DÃ©marrage â€” Objectif : 3000 annonces\n",
      "\n",
      "   Structure images : media/{property_type}/{id}/image_001.jpg\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ“¦  APARTMENT  |  SALE\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  ğŸ“„ Page 1 â†’ https://www.mubawab.tn/fr/sc/appartements-a-vendre\n",
      "      â†’ 34 annonces [sel: 'div.listingBox']\n",
      "      â¬‡ï¸  [1/300] S2 en vente...\n",
      "         âœ… id=1 | rooms=2 | surface=NonemÂ² | imgs=17 | city=Tunisie\n",
      "      â¬‡ï¸  [2/300] S2 en vente...\n",
      "         âœ… id=2 | rooms=2 | surface=131.39mÂ² | imgs=22 | city=Tunisie\n",
      "      â¬‡ï¸  [3/300] S3 en vente...\n",
      "         âœ… id=3 | rooms=3 | surface=NonemÂ² | imgs=19 | city=Tunisie\n",
      "      â¬‡ï¸  [4/300] Fabuleux appartement S2 Ã  vendre...\n",
      "         âœ… id=4 | rooms=2 | surface=131.0mÂ² | imgs=13 | city=Tunisie\n",
      "      â¬‡ï¸  [5/300] Appartement Ã  vendre Ã  Les Jardins d'El Menzah 2...\n",
      "         âœ… id=5 | rooms=None | surface=67.0mÂ² | imgs=8 | city=Tunisie\n",
      "      â¬‡ï¸  [6/300] Appartement S1 RDC Ã  vendre, RÃ©sidence MALAGA 2 ...\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cur  = conn.cursor()\n",
    "driver = create_driver()\n",
    "\n",
    "total_saved    = 0\n",
    "category_stats = {}\n",
    "\n",
    "print(\"ğŸš€ DÃ©marrage â€” Objectif : 3000 annonces\\n\")\n",
    "print(f\"   Structure images : {MEDIA_ROOT}/{{property_type}}/{{id}}/image_001.jpg\\n\")\n",
    "\n",
    "try:\n",
    "    for slug, (trans_type, prop_type) in CATEGORIES.items():\n",
    "        cat_key = f\"{prop_type}_{trans_type}\"\n",
    "        cat_saved = 0\n",
    "\n",
    "        print(f\"\\n{'â•'*60}\")\n",
    "        print(f\"  ğŸ“¦  {prop_type.upper()}  |  {trans_type.upper()}\")\n",
    "        print(f\"{'â•'*60}\")\n",
    "\n",
    "        for page_num in range(1, MAX_PAGES_PER_CATEGORY + 1):\n",
    "\n",
    "            if cat_saved >= TARGET_PER_CATEGORY:\n",
    "                print(f\"  ğŸ¯ Objectif {TARGET_PER_CATEGORY} atteint.\")\n",
    "                break\n",
    "\n",
    "            url_page = (\n",
    "                f\"https://www.mubawab.tn/fr/{slug}\"\n",
    "                if page_num == 1\n",
    "                else f\"https://www.mubawab.tn/fr/{slug}:p:{page_num}\"\n",
    "            )\n",
    "            print(f\"\\n  ğŸ“„ Page {page_num} â†’ {url_page}\")\n",
    "\n",
    "            listings_raw, used_sel = scrape_list_page(driver, url_page)\n",
    "\n",
    "            if not listings_raw:\n",
    "                print(\"  ğŸ›‘ Aucune annonce. Fin de catÃ©gorie.\")\n",
    "                break\n",
    "\n",
    "            print(f\"      â†’ {len(listings_raw)} annonces [sel: '{used_sel}']\")\n",
    "\n",
    "            for item in listings_raw:\n",
    "\n",
    "                if cat_saved >= TARGET_PER_CATEGORY:\n",
    "                    break\n",
    "\n",
    "                basic = extract_basic(item, trans_type, prop_type)\n",
    "                if not basic:\n",
    "                    continue\n",
    "\n",
    "                # Anti-doublon\n",
    "                cur.execute(\"SELECT id FROM mubawab_listings WHERE url=%s\", (basic[\"url\"],))\n",
    "                if cur.fetchone():\n",
    "                    continue\n",
    "\n",
    "                print(f\"      â¬‡ï¸  [{cat_saved+1}/{TARGET_PER_CATEGORY}] \"\n",
    "                      f\"{basic['title'][:48]}...\")\n",
    "\n",
    "                # â”€â”€ Page dÃ©tail â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                det = scrape_detail(driver, basic[\"url\"])\n",
    "\n",
    "                title       = basic[\"title\"]\n",
    "                description = det.get(\"description\", \"\")\n",
    "\n",
    "                # â”€â”€ Extraction intelligente rooms & surface â”€â”€â”€\n",
    "                rooms, surface = smart_extract(title, description)\n",
    "\n",
    "                # Fallback : si pas trouvÃ© dans texte, essayer bedrooms brut\n",
    "                if not rooms and det.get(\"bedrooms\"):\n",
    "                    rooms = extract_rooms(det[\"bedrooms\"])\n",
    "\n",
    "                # â”€â”€ GÃ©ographie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                loc_dict, municipality = build_location(basic[\"subtitle\"])\n",
    "\n",
    "                # â”€â”€ Insertion (RETURNING id) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                try:\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO mubawab_listings\n",
    "                            (title, subtitle, price,\n",
    "                             transaction_type, property_type,\n",
    "                             location_details, municipality,\n",
    "                             surface, rooms,\n",
    "                             bedrooms, bathrooms,\n",
    "                             features, poi, description, url,\n",
    "                             image_path, images_folder, images_count,\n",
    "                             last_updated)\n",
    "                        VALUES\n",
    "                            (%s,%s,%s, %s,%s, %s,%s, %s,%s, %s,%s,\n",
    "                             %s,%s,%s,%s, %s,%s,%s, %s)\n",
    "                        ON CONFLICT (url) DO NOTHING\n",
    "                        RETURNING id\n",
    "                    \"\"\", (\n",
    "                        title,\n",
    "                        basic[\"subtitle\"],\n",
    "                        basic[\"price\"],\n",
    "                        trans_type,\n",
    "                        prop_type,\n",
    "                        json.dumps(loc_dict),\n",
    "                        municipality,\n",
    "                        surface,                           # FLOAT ou None\n",
    "                        rooms,                             # INTEGER ou None\n",
    "                        det.get(\"bedrooms\"),\n",
    "                        det.get(\"bathrooms\"),\n",
    "                        json.dumps(det.get(\"features\", {})),\n",
    "                        json.dumps(det.get(\"poi\", {})),\n",
    "                        description,\n",
    "                        basic[\"url\"],\n",
    "                        None, None, 0,                     # images remplies aprÃ¨s\n",
    "                        det.get(\"last_updated\",\"Unknown\"),\n",
    "                    ))\n",
    "                    conn.commit()\n",
    "\n",
    "                    row = cur.fetchone()\n",
    "                    if not row:\n",
    "                        continue\n",
    "                    listing_id = row[0]\n",
    "\n",
    "                except Exception as db_err:\n",
    "                    print(f\"      âŒ DB: {db_err}\")\n",
    "                    conn.rollback()\n",
    "                    continue\n",
    "\n",
    "                # â”€â”€ TÃ©lÃ©chargement images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                img_urls = det.get(\"img_urls\", [])\n",
    "                if basic.get(\"thumb\") and basic[\"thumb\"] not in img_urls:\n",
    "                    img_urls.insert(0, basic[\"thumb\"])\n",
    "\n",
    "                thumb_path, folder, nb_imgs = download_images(\n",
    "                    img_urls, prop_type, listing_id\n",
    "                )\n",
    "\n",
    "                # Mise Ã  jour images\n",
    "                cur.execute(\"\"\"\n",
    "                    UPDATE mubawab_listings\n",
    "                    SET image_path=%s, images_folder=%s, images_count=%s\n",
    "                    WHERE id=%s\n",
    "                \"\"\", (thumb_path, folder, nb_imgs, listing_id))\n",
    "                conn.commit()\n",
    "\n",
    "                cat_saved   += 1\n",
    "                total_saved += 1\n",
    "\n",
    "                print(f\"         âœ… id={listing_id} | \"\n",
    "                      f\"rooms={rooms} | surface={surface}mÂ² | \"\n",
    "                      f\"imgs={nb_imgs} | city={loc_dict.get('city')}\")\n",
    "\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "\n",
    "        category_stats[cat_key] = cat_saved\n",
    "        print(f\"\\n  ğŸ“Š {cat_key} â†’ {cat_saved} annonces enregistrÃ©es\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸ Interrompu manuellement (donnÃ©es dÃ©jÃ  commitÃ©es).\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"\\n{'â•'*60}\")\n",
    "    print(f\"  ğŸ‰ TERMINÃ‰ â€” {total_saved} annonces au total\")\n",
    "    print(f\"{'â•'*60}\")\n",
    "    for k, v in category_stats.items():\n",
    "        print(f\"  {k:<35} : {v:>4}\")\n",
    "    print(f\"\\n  ğŸ“ Images â†’ {MEDIA_ROOT}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710483d-1ce6-4554-8648-dfc338a0cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "print(\"â•\"*60)\n",
    "print(\"  ğŸ“‹ APERÃ‡U DES DONNÃ‰ES\")\n",
    "print(\"â•\"*60)\n",
    "\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        title,\n",
    "        transaction_type   AS type,\n",
    "        property_type      AS bien,\n",
    "        rooms,\n",
    "        surface,\n",
    "        price,\n",
    "        municipality,\n",
    "        location_details->>'zone'  AS zone,\n",
    "        images_count,\n",
    "        scraped_at::date   AS date\n",
    "    FROM mubawab_listings\n",
    "    ORDER BY scraped_at DESC\n",
    "    LIMIT 15\n",
    "\"\"\", conn)\n",
    "display(df)\n",
    "\n",
    "print(\"\\n\" + \"â•\"*60)\n",
    "print(\"  ğŸ“Š STATISTIQUES PAR CATÃ‰GORIE\")\n",
    "print(\"â•\"*60)\n",
    "\n",
    "stats = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        property_type                           AS bien,\n",
    "        transaction_type                        AS type,\n",
    "        COUNT(*)                                AS total,\n",
    "        COUNT(rooms)                            AS avec_rooms,\n",
    "        COUNT(surface)                          AS avec_surface,\n",
    "        ROUND(AVG(rooms)::numeric, 1)           AS rooms_moy,\n",
    "        ROUND(AVG(surface)::numeric, 1)         AS surface_moy,\n",
    "        SUM(images_count)                       AS total_images\n",
    "    FROM mubawab_listings\n",
    "    GROUP BY property_type, transaction_type\n",
    "    ORDER BY bien, type\n",
    "\"\"\", conn)\n",
    "display(stats)\n",
    "\n",
    "print(\"\\n\" + \"â•\"*60)\n",
    "print(\"  ğŸ§ª TEST EXTRACTION INTELLIGENTE (20 exemples)\")\n",
    "print(\"â•\"*60)\n",
    "\n",
    "sample = pd.read_sql(\"\"\"\n",
    "    SELECT title, rooms, surface\n",
    "    FROM mubawab_listings\n",
    "    WHERE rooms IS NOT NULL OR surface IS NOT NULL\n",
    "    LIMIT 20\n",
    "\"\"\", conn)\n",
    "display(sample)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
