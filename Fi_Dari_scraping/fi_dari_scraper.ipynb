{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce28f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\chtou\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\chtou\\anaconda3\\lib\\site-packages (2.9.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: selenium in c:\\users\\chtou\\anaconda3\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\chtou\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\chtou\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chtou\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: trio-typing>=0.10.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (0.10.0)\n",
      "Requirement already satisfied: types-certifi>=2021.10.8.3 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8.3)\n",
      "Requirement already satisfied: types-urllib3>=1.26.25.14 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (1.26.25.14)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.2 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio-typing>=0.10.0->selenium) (1.0.0)\n",
      "Requirement already satisfied: async-generator in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio-typing>=0.10.0->selenium) (1.10)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from trio-typing>=0.10.0->selenium) (8.5.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\chtou\\anaconda3\\lib\\site-packages (from importlib-metadata->trio-typing>=0.10.0->selenium) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests psycopg2-binary beautifulsoup4 selenium webdriver-manager pandas Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe27f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cellule 2 — Configuration chargée\n",
      "   8 catégories | TARGET=200/catégorie\n",
      "   116 villes/délégations indexées\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 2 — IMPORTS & CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "import os, re, time, json, random, requests, psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\":     \"localhost\",\n",
    "    \"database\": \"estate_mind_db\",\n",
    "    \"user\":     \"postgres\",\n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "MEDIA_ROOT        = r\"C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\"\n",
    "TARGET_PER_TYPE   = 200\n",
    "MAX_PAGES_INITIAL = 20\n",
    "MAX_PAGES_DAILY   = 3\n",
    "BASE_URL          = \"https://fi-dari.tn\"\n",
    "B                 = \"[[37.649,7.778],[30.107,11.953]]\"\n",
    "\n",
    "CATEGORIES = {\n",
    "    f\"search?objectif=vendre&categorie=Appartement&usage=Habitation&bounds={B}\":           (\"sale\", \"apartment\"),\n",
    "    f\"search?objectif=louer&categorie=Appartement&usage=Tout+type+de+location&bounds={B}\": (\"rent\", \"apartment\"),\n",
    "    f\"search?objectif=vendre&categorie=Maison&usage=Habitation&bounds={B}\":                (\"sale\", \"house\"),\n",
    "    f\"search?objectif=louer&categorie=Maison&usage=Tout+type+de+location&bounds={B}\":      (\"rent\", \"house\"),\n",
    "    f\"search?objectif=vendre&categorie=Terrain&bounds={B}\":                                (\"sale\", \"land\"),\n",
    "    f\"search?objectif=louer&categorie=Terrain&bounds={B}\":                                 (\"rent\", \"land\"),\n",
    "    f\"search?objectif=vendre&categorie=Bureau&usage=Professionnels&bounds={B}\":            (\"sale\", \"office\"),\n",
    "    f\"search?objectif=louer&categorie=Bureau&usage=Bureaux+et+commerce&bounds={B}\":        (\"rent\", \"office\"),\n",
    "}\n",
    "\n",
    "PROPERTY_CONFIG = {\n",
    "    \"apartment\": {\"has_rooms\": True,  \"has_surface\": True},\n",
    "    \"house\":     {\"has_rooms\": True,  \"has_surface\": True},\n",
    "    \"land\":      {\"has_rooms\": False, \"has_surface\": True},\n",
    "    \"office\":    {\"has_rooms\": False, \"has_surface\": True},\n",
    "}\n",
    "\n",
    "# ── Carte géographique complète Tunisie ─────────────────────────────────────\n",
    "# Chaque ville/délégation est mappée vers (zone, gouvernorat_officiel)\n",
    "CITY_TO_GEO = {\n",
    "    # Grand Tunis\n",
    "    \"tunis\":            (\"grand_tunis\", \"Tunis\"),\n",
    "    \"ariana\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"ben arous\":        (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"manouba\":          (\"grand_tunis\", \"Manouba\"),\n",
    "    \"la marsa\":         (\"grand_tunis\", \"Tunis\"),\n",
    "    \"marsa\":            (\"grand_tunis\", \"Tunis\"),\n",
    "    \"carthage\":         (\"grand_tunis\", \"Tunis\"),\n",
    "    \"la goulette\":      (\"grand_tunis\", \"Tunis\"),\n",
    "    \"bardo\":            (\"grand_tunis\", \"Tunis\"),\n",
    "    \"ezzahra\":          (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"hammam lif\":       (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"hammam chatt\":     (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"bou mhel\":         (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"el aouina\":        (\"grand_tunis\", \"Tunis\"),\n",
    "    \"el menzah\":        (\"grand_tunis\", \"Ariana\"),\n",
    "    \"soukra\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"raoued\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"ennasr\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"mrezga\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"centre urbain\":    (\"grand_tunis\", \"Tunis\"),\n",
    "    \"lac 1\":            (\"grand_tunis\", \"Tunis\"),\n",
    "    \"lac 2\":            (\"grand_tunis\", \"Tunis\"),\n",
    "    \"les berges du lac\":(\"grand_tunis\", \"Tunis\"),\n",
    "    \"ain zaghouan\":     (\"grand_tunis\", \"Tunis\"),\n",
    "    \"el ghazela\":       (\"grand_tunis\", \"Ariana\"),\n",
    "    \"borj louzir\":      (\"grand_tunis\", \"Ariana\"),\n",
    "    \"mnihla\":           (\"grand_tunis\", \"Ariana\"),\n",
    "    \"kalaat landlous\":  (\"grand_tunis\", \"Manouba\"),\n",
    "    \"douar hicher\":     (\"grand_tunis\", \"Manouba\"),\n",
    "    \"den den\":          (\"grand_tunis\", \"Manouba\"),\n",
    "    \"oued ellil\":       (\"grand_tunis\", \"Manouba\"),\n",
    "    \"jedaida\":          (\"grand_tunis\", \"Manouba\"),\n",
    "    \"tebourba\":         (\"grand_tunis\", \"Manouba\"),\n",
    "    \"mourouj\":          (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"medina jadida\":    (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"megrine\":          (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"hammam el ghezaz\": (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"fouchana\":         (\"grand_tunis\", \"Ben Arous\"),\n",
    "    \"mornag\":           (\"grand_tunis\", \"Ben Arous\"),\n",
    "    # Nord\n",
    "    \"bizerte\":          (\"nord\", \"Bizerte\"),\n",
    "    \"nabeul\":           (\"nord\", \"Nabeul\"),\n",
    "    \"hammamet\":         (\"nord\", \"Nabeul\"),\n",
    "    \"beja\":             (\"nord\", \"Béja\"),\n",
    "    \"jendouba\":         (\"nord\", \"Jendouba\"),\n",
    "    \"zaghouan\":         (\"nord\", \"Zaghouan\"),\n",
    "    \"kelibia\":          (\"nord\", \"Nabeul\"),\n",
    "    \"korba\":            (\"nord\", \"Nabeul\"),\n",
    "    \"menzel bourguiba\": (\"nord\", \"Bizerte\"),\n",
    "    \"grombalia\":        (\"nord\", \"Nabeul\"),\n",
    "    \"ain draham\":       (\"nord\", \"Jendouba\"),\n",
    "    \"tabarka\":          (\"nord\", \"Jendouba\"),\n",
    "    \"menzel temime\":    (\"nord\", \"Nabeul\"),\n",
    "    \"el haouaria\":      (\"nord\", \"Nabeul\"),\n",
    "    \"dar chaabane\":     (\"nord\", \"Nabeul\"),\n",
    "    \"beni khiar\":       (\"nord\", \"Nabeul\"),\n",
    "    \"kélibia\":          (\"nord\", \"Nabeul\"),\n",
    "    \"soliman\":          (\"nord\", \"Nabeul\"),\n",
    "    \"bou argoub\":       (\"nord\", \"Nabeul\"),\n",
    "    \"menzel bouzaiane\": (\"nord\", \"Nabeul\"),\n",
    "    \"takelsa\":          (\"nord\", \"Nabeul\"),\n",
    "    # Sahel\n",
    "    \"sousse\":           (\"sahel\", \"Sousse\"),\n",
    "    \"monastir\":         (\"sahel\", \"Monastir\"),\n",
    "    \"mahdia\":           (\"sahel\", \"Mahdia\"),\n",
    "    \"msaken\":           (\"sahel\", \"Sousse\"),\n",
    "    \"el jem\":           (\"sahel\", \"Mahdia\"),\n",
    "    \"hergla\":           (\"sahel\", \"Sousse\"),\n",
    "    \"akouda\":           (\"sahel\", \"Sousse\"),\n",
    "    \"kantaoui\":         (\"sahel\", \"Sousse\"),\n",
    "    \"port el kantaoui\": (\"sahel\", \"Sousse\"),\n",
    "    \"kalaa kebira\":     (\"sahel\", \"Sousse\"),\n",
    "    \"kalaa sghira\":     (\"sahel\", \"Sousse\"),\n",
    "    \"enfidha\":          (\"sahel\", \"Sousse\"),\n",
    "    \"hamammet\":         (\"sahel\", \"Nabeul\"),\n",
    "    \"ksar hellal\":      (\"sahel\", \"Monastir\"),\n",
    "    \"moknine\":          (\"sahel\", \"Monastir\"),\n",
    "    \"bekalta\":          (\"sahel\", \"Monastir\"),\n",
    "    \"jemmal\":           (\"sahel\", \"Monastir\"),\n",
    "    \"bembla\":           (\"sahel\", \"Monastir\"),\n",
    "    \"bou merdes\":       (\"sahel\", \"Monastir\"),\n",
    "    \"chebba\":           (\"sahel\", \"Mahdia\"),\n",
    "    \"ksour essef\":      (\"sahel\", \"Mahdia\"),\n",
    "    # Centre\n",
    "    \"kairouan\":         (\"centre\", \"Kairouan\"),\n",
    "    \"kasserine\":        (\"centre\", \"Kasserine\"),\n",
    "    \"sidi bouzid\":      (\"centre\", \"Sidi Bouzid\"),\n",
    "    \"siliana\":          (\"centre\", \"Siliana\"),\n",
    "    \"gafsa\":            (\"centre\", \"Gafsa\"),\n",
    "    \"sbeitla\":          (\"centre\", \"Kasserine\"),\n",
    "    \"sbiba\":            (\"centre\", \"Kasserine\"),\n",
    "    \"thala\":            (\"centre\", \"Kasserine\"),\n",
    "    # Sud\n",
    "    \"sfax\":             (\"sud\", \"Sfax\"),\n",
    "    \"gabes\":            (\"sud\", \"Gabès\"),\n",
    "    \"gabès\":            (\"sud\", \"Gabès\"),\n",
    "    \"jerba\":            (\"sud\", \"Médenine\"),\n",
    "    \"djerba\":           (\"sud\", \"Médenine\"),\n",
    "    \"medenine\":         (\"sud\", \"Médenine\"),\n",
    "    \"médenine\":         (\"sud\", \"Médenine\"),\n",
    "    \"tataouine\":        (\"sud\", \"Tataouine\"),\n",
    "    \"tozeur\":           (\"sud\", \"Tozeur\"),\n",
    "    \"kebili\":           (\"sud\", \"Kébili\"),\n",
    "    \"kébili\":           (\"sud\", \"Kébili\"),\n",
    "    \"zarzis\":           (\"sud\", \"Médenine\"),\n",
    "    \"ben gardane\":      (\"sud\", \"Médenine\"),\n",
    "    \"el hamma\":         (\"sud\", \"Gabès\"),\n",
    "    \"matmata\":          (\"sud\", \"Gabès\"),\n",
    "    \"ghomrassen\":       (\"sud\", \"Tataouine\"),\n",
    "    \"remada\":           (\"sud\", \"Tataouine\"),\n",
    "    \"ghoumrassen\":      (\"sud\", \"Tataouine\"),\n",
    "    \"nefta\":            (\"sud\", \"Tozeur\"),\n",
    "    \"douz\":             (\"sud\", \"Kébili\"),\n",
    "    \"sfax sud\":         (\"sud\", \"Sfax\"),\n",
    "    \"sfax ville\":       (\"sud\", \"Sfax\"),\n",
    "    \"thyna\":            (\"sud\", \"Sfax\"),\n",
    "    \"sakiet eddaier\":   (\"sud\", \"Sfax\"),\n",
    "    \"sakiet ezzit\":     (\"sud\", \"Sfax\"),\n",
    "    \"el ain\":           (\"sud\", \"Sfax\"),\n",
    "    \"agareb\":           (\"sud\", \"Sfax\"),\n",
    "}\n",
    "\n",
    "# Table gouvernorat → zone (fallback)\n",
    "GOUVERNORAT_ZONE = {\n",
    "    \"Tunis\":      \"grand_tunis\", \"Ariana\":    \"grand_tunis\",\n",
    "    \"Ben Arous\":  \"grand_tunis\", \"Manouba\":   \"grand_tunis\",\n",
    "    \"Bizerte\":    \"nord\",        \"Nabeul\":    \"nord\",\n",
    "    \"Béja\":       \"nord\",        \"Jendouba\":  \"nord\",\n",
    "    \"Zaghouan\":   \"nord\",\n",
    "    \"Sousse\":     \"sahel\",       \"Monastir\":  \"sahel\",\n",
    "    \"Mahdia\":     \"sahel\",\n",
    "    \"Kairouan\":   \"centre\",      \"Kasserine\": \"centre\",\n",
    "    \"Sidi Bouzid\":\"centre\",      \"Siliana\":   \"centre\",\n",
    "    \"Gafsa\":      \"centre\",\n",
    "    \"Sfax\":       \"sud\",         \"Gabès\":     \"sud\",\n",
    "    \"Médenine\":   \"sud\",         \"Tataouine\": \"sud\",\n",
    "    \"Tozeur\":     \"sud\",         \"Kébili\":    \"sud\",\n",
    "}\n",
    "\n",
    "FEATURE_MAPPING = {\n",
    "    \"Climatisation\":\"has_air_conditioning\",\"Climatiseur\":\"has_air_conditioning\",\n",
    "    \"Chauffage central\":\"has_central_heating\",\"Chauffage\":\"has_heating\",\n",
    "    \"Ascenseur\":\"has_elevator\",\"Jardin\":\"has_garden\",\"Piscine\":\"has_pool\",\n",
    "    \"Terrasse\":\"has_terrace\",\"Garage\":\"has_garage\",\"Parking\":\"has_parking\",\n",
    "    \"Parking sous-sol\":\"has_underground_parking\",\"Parking extérieur\":\"has_outdoor_parking\",\n",
    "    \"Vue sur mer\":\"has_sea_view\",\"Meublé\":\"is_furnished\",\n",
    "    \"Cuisine équipée\":\"has_equipped_kitchen\",\"Cuisine indépendante\":\"has_separate_kitchen\",\n",
    "    \"Interphone\":\"has_intercom\",\"Balcon\":\"has_balcony\",\"Cave\":\"has_cellar\",\n",
    "    \"Sécurité\":\"has_security\",\"Gardien\":\"has_caretaker\",\n",
    "    \"Caméra de sécurité\":\"has_cctv\",\"Vidéosurveillance\":\"has_cctv\",\n",
    "    \"Double vitrage\":\"has_double_glazing\",\"Fibre optique\":\"has_fiber_optic\",\n",
    "    \"Digicode\":\"has_digicode\",\"Porte blindée\":\"has_armored_door\",\n",
    "    \"Lumineux\":\"is_bright\",\"Calme\":\"is_quiet\",\n",
    "}\n",
    "\n",
    "os.makedirs(MEDIA_ROOT, exist_ok=True)\n",
    "print(\"✅ Cellule 2 — Configuration chargée\")\n",
    "print(f\"   {len(CATEGORIES)} catégories | TARGET={TARGET_PER_TYPE}/catégorie\")\n",
    "print(f\"   {len(CITY_TO_GEO)} villes/délégations indexées\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6798bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tables créées (sans subtitle, sans price_raw).\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 3 — BASE DE DONNÉES  (sans subtitle, sans price_raw)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def get_conn():\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def create_tables():\n",
    "    conn = get_conn(); cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"DROP TABLE IF EXISTS fi_dari_listings CASCADE;\")\n",
    "        cur.execute(\"DROP TABLE IF EXISTS fi_dari_log CASCADE;\")\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE fi_dari_listings (\n",
    "                id               SERIAL PRIMARY KEY,\n",
    "                title            TEXT,\n",
    "                price            REAL,\n",
    "                transaction_type VARCHAR(20),\n",
    "                type             VARCHAR(30),\n",
    "                region           VARCHAR(100),\n",
    "                municipality     VARCHAR(100),\n",
    "                zone             VARCHAR(50),\n",
    "                location_details JSONB,\n",
    "                surface          REAL,\n",
    "                rooms            INTEGER,\n",
    "                description      TEXT,\n",
    "                features         JSONB,\n",
    "                poi              JSONB,\n",
    "                image_path       TEXT,\n",
    "                images_folder    TEXT,\n",
    "                images_count     INTEGER DEFAULT 0,\n",
    "                url              TEXT UNIQUE NOT NULL,\n",
    "                pdf_link         TEXT,\n",
    "                last_updated     TEXT,\n",
    "                scraped_at       TIMESTAMP DEFAULT NOW(),\n",
    "                is_new           BOOLEAN DEFAULT TRUE\n",
    "            );\n",
    "        \"\"\")\n",
    "        for sql in [\n",
    "            \"CREATE INDEX idx_fd_type        ON fi_dari_listings(type);\",\n",
    "            \"CREATE INDEX idx_fd_transaction ON fi_dari_listings(transaction_type);\",\n",
    "            \"CREATE INDEX idx_fd_region      ON fi_dari_listings(region);\",\n",
    "            \"CREATE INDEX idx_fd_zone        ON fi_dari_listings(zone);\",\n",
    "            \"CREATE INDEX idx_fd_price       ON fi_dari_listings(price);\",\n",
    "            \"CREATE INDEX idx_fd_scraped     ON fi_dari_listings(scraped_at);\",\n",
    "            \"CREATE INDEX idx_fd_url         ON fi_dari_listings(url);\",\n",
    "            \"CREATE INDEX idx_is_new         ON fi_dari_listings(is_new);\",\n",
    "        ]:\n",
    "            cur.execute(sql)\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE fi_dari_log (\n",
    "                id           SERIAL PRIMARY KEY,\n",
    "                run_type     VARCHAR(20),\n",
    "                started_at   TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                finished_at  TIMESTAMP,\n",
    "                total_new    INTEGER DEFAULT 0,\n",
    "                total_errors INTEGER DEFAULT 0,\n",
    "                status       VARCHAR(20) DEFAULT 'running'\n",
    "            );\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        print(\"✅ Tables créées (sans subtitle, sans price_raw).\")\n",
    "    except Exception as e:\n",
    "        conn.rollback(); print(f\"❌ Erreur SQL : {e}\")\n",
    "    finally:\n",
    "        cur.close(); conn.close()\n",
    "\n",
    "def url_exists(cur, url):\n",
    "    cur.execute(\"SELECT 1 FROM fi_dari_listings WHERE url=%s LIMIT 1\", (url,))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "def insert_listing(cur, d):\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO fi_dari_listings\n",
    "                (title, price, transaction_type, type,\n",
    "                 region, municipality, zone, location_details,\n",
    "                 surface, rooms, features, poi, description,\n",
    "                 url, pdf_link, image_path, images_folder,\n",
    "                 images_count, last_updated, is_new)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            ON CONFLICT(url) DO NOTHING RETURNING id\n",
    "        \"\"\", (\n",
    "            d.get(\"title\"), d.get(\"price\"),\n",
    "            d.get(\"transaction_type\"), d.get(\"type\"),\n",
    "            d.get(\"region\"), d.get(\"municipality\"), d.get(\"zone\"),\n",
    "            json.dumps(d.get(\"location_details\", {}), ensure_ascii=False),\n",
    "            d.get(\"surface\"), d.get(\"rooms\"),\n",
    "            json.dumps(d.get(\"features\", {}), ensure_ascii=False),\n",
    "            json.dumps(d.get(\"poi\",      {}), ensure_ascii=False),\n",
    "            d.get(\"description\", \"\"), d.get(\"url\"), d.get(\"pdf_link\"),\n",
    "            None, None, 0, d.get(\"last_updated\", \"Unknown\"), True,\n",
    "        ))\n",
    "        row = cur.fetchone()\n",
    "        return row[0] if row else None\n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ Insert: {e}\"); return None\n",
    "\n",
    "def update_images(cur, listing_id, thumb, folder, count):\n",
    "    cur.execute(\"\"\"\n",
    "        UPDATE fi_dari_listings\n",
    "        SET image_path=%s, images_folder=%s, images_count=%s\n",
    "        WHERE id=%s\n",
    "    \"\"\", (thumb, folder, count, listing_id))\n",
    "\n",
    "def log_start(cur, run_type):\n",
    "    cur.execute(\"INSERT INTO fi_dari_log(run_type) VALUES(%s) RETURNING id\", (run_type,))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def log_end(cur, log_id, n, e, status=\"done\"):\n",
    "    cur.execute(\"\"\"\n",
    "        UPDATE fi_dari_log\n",
    "        SET finished_at=NOW(), total_new=%s, total_errors=%s, status=%s\n",
    "        WHERE id=%s\n",
    "    \"\"\", (n, e, status, log_id))\n",
    "\n",
    "def reset_is_new(cur):\n",
    "    cur.execute(\"UPDATE fi_dari_listings SET is_new=FALSE\")\n",
    "\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f526b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cellule 4 — Utilitaires chargés\n",
      "   Géolocalisation intelligente : 112 villes indexées\n",
      "   Filtre images : URL + extension + taille + dimensions\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 4 — UTILITAIRES\n",
    "# Corrections :\n",
    "#   - Localisation INTELLIGENTE (titre+desc+balises, pas \"Tunisie\" par défaut)\n",
    "#   - Images : filtre strict URL + taille + dimensions\n",
    "#   - Suppression subtitle partout\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def _normalize(s):\n",
    "    \"\"\"Minuscule + supprime accents pour comparaison robuste.\"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", s.lower()).encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "# Index normalisé pour la recherche géo\n",
    "_CITY_NORM = {_normalize(k): v for k, v in CITY_TO_GEO.items()}\n",
    "\n",
    "def _find_city_in_text(text):\n",
    "    \"\"\"\n",
    "    Cherche la première ville connue dans un texte quelconque.\n",
    "    Retourne (zone, region, city_name) ou None.\n",
    "    Tri par longueur décroissante pour préférer les noms longs.\n",
    "    \"\"\"\n",
    "    if not text: return None\n",
    "    t = _normalize(text)\n",
    "    for city_norm in sorted(_CITY_NORM, key=len, reverse=True):\n",
    "        pat = r'\\b' + re.escape(city_norm) + r'\\b'\n",
    "        if re.search(pat, t):\n",
    "            zone, region = _CITY_NORM[city_norm]\n",
    "            orig = next(k for k in CITY_TO_GEO if _normalize(k) == city_norm)\n",
    "            return zone, region, orig.title()\n",
    "    return None\n",
    "\n",
    "def _extract_municipality_clean(text, city_name):\n",
    "    \"\"\"\n",
    "    Extrait la municipalité propre depuis un texte (1-3 mots max).\n",
    "    Ex: \"Achat appartement s+1 76m² Hammamet centre\" → \"Hammamet Centre\"\n",
    "    Ex: \"Hammamet - Nabeul\" → \"Hammamet\"\n",
    "    Ex: \"La Marsa, Ariana\" → \"La Marsa\"\n",
    "    \"\"\"\n",
    "    if not text or not city_name: return city_name\n",
    "    t = _normalize(text)\n",
    "    city_n = _normalize(city_name)\n",
    "    # Chercher la ville dans le texte\n",
    "    m = re.search(r'\\b' + re.escape(city_n) + r'\\b', t)\n",
    "    if not m: return city_name\n",
    "    # Prendre ce qui suit immédiatement la ville (qualificatif optionnel)\n",
    "    after = t[m.end():m.end()+30].strip()\n",
    "    # Qualifier autorisé : nord, sud, est, ouest, centre, ville, plage, corniche, etc.\n",
    "    qualifiers = ['nord','sud','est','ouest','centre','ville','plage','corniche',\n",
    "                  'centre ville','sud ouest','nord est','nord ouest','sud est',\n",
    "                  'la marsa','el menzah','el mourouj','el ghazela','berges du lac',\n",
    "                  'lac 1','lac 2','cite','cité']\n",
    "    extra = ''\n",
    "    for q in sorted(qualifiers, key=len, reverse=True):\n",
    "        if after.startswith(q):\n",
    "            extra = ' ' + q.title()\n",
    "            break\n",
    "    # Reconstuire avec casse propre\n",
    "    result = city_name + extra\n",
    "    return result.strip()\n",
    "\n",
    "def build_location(raw_loc, title=\"\", description=\"\", soup=None):\n",
    "    \"\"\"\n",
    "    Extraction géographique intelligente — 5 étapes par priorité :\n",
    "    1. Balises HTML spécifiques (breadcrumb, localisation)\n",
    "    2. raw_loc (texte brut de la page)\n",
    "    3. Titre de l'annonce\n",
    "    4. Description\n",
    "    5. URL du bien (dernier recours)\n",
    "    Retourne dict avec region, municipality, zone, location_details.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # ── Étape 1 : balises HTML ─────────────────────────────────\n",
    "    if soup:\n",
    "        # Breadcrumb ou span localisation\n",
    "        for tag in soup.find_all([\"span\",\"p\",\"div\",\"li\",\"a\"],\n",
    "                                  class_=re.compile(r'local|address|ville|region|breadcrumb|lieu|where|city', re.I)):\n",
    "            t = tag.get_text(\" \", strip=True)\n",
    "            if 2 < len(t) < 150: candidates.append(t)\n",
    "        # Balises meta\n",
    "        for meta in soup.find_all(\"meta\"):\n",
    "            prop = (meta.get(\"property\",\"\") + meta.get(\"name\",\"\")).lower()\n",
    "            if any(k in prop for k in [\"locality\",\"city\",\"region\",\"address\"]):\n",
    "                c = meta.get(\"content\",\"\").strip()\n",
    "                if c: candidates.append(c)\n",
    "\n",
    "    # ── Étape 2 : raw_loc ─────────────────────────────────────\n",
    "    if raw_loc and raw_loc.lower() not in (\"tunisie\", \"\"):\n",
    "        candidates.insert(0, raw_loc)\n",
    "\n",
    "    # ── Étape 3 & 4 : titre + description ────────────────────\n",
    "    candidates.append(title)\n",
    "    if description: candidates.append(description[:500])\n",
    "\n",
    "    # ── Recherche dans chaque candidat ───────────────────────\n",
    "    found = None\n",
    "    municipality_raw = \"\"\n",
    "    for cand in candidates:\n",
    "        res = _find_city_in_text(cand)\n",
    "        if res:\n",
    "            zone, region, city_name = res\n",
    "            found = (zone, region, city_name)\n",
    "            # Extraire la municipalité propre (nom de ville + qualificatif max)\n",
    "            municipality_raw = _extract_municipality_clean(cand, city_name)\n",
    "            break\n",
    "\n",
    "    if found:\n",
    "        zone, region, city_name = found\n",
    "        municipality = municipality_raw or city_name\n",
    "        location_details = {\n",
    "            \"region\": region, \"municipality\": municipality,\n",
    "            \"zone\": zone, \"raw\": raw_loc or title\n",
    "        }\n",
    "        return {\n",
    "            \"region\": region, \"municipality\": municipality,\n",
    "            \"zone\": zone, \"location_details\": location_details\n",
    "        }\n",
    "\n",
    "    # ── Fallback : parse raw_loc basique ─────────────────────\n",
    "    parts = [p.strip() for p in (raw_loc or \"\").replace(\" - \", \",\").split(\",\") if p.strip()]\n",
    "    region_raw = parts[-1] if parts else \"Tunisie\"\n",
    "    municipality_raw = parts[-2] if len(parts) >= 2 else (parts[0] if parts else \"Tunisie\")\n",
    "\n",
    "    # Chercher le gouvernorat dans GOUVERNORAT_ZONE\n",
    "    zone = \"autre\"\n",
    "    for gov, z in GOUVERNORAT_ZONE.items():\n",
    "        if _normalize(gov) in _normalize(region_raw):\n",
    "            zone = z; break\n",
    "\n",
    "    location_details = {\n",
    "        \"region\": region_raw, \"municipality\": municipality_raw,\n",
    "        \"zone\": zone, \"raw\": raw_loc or \"Tunisie\"\n",
    "    }\n",
    "    return {\n",
    "        \"region\": region_raw, \"municipality\": municipality_raw,\n",
    "        \"zone\": zone, \"location_details\": location_details\n",
    "    }\n",
    "\n",
    "def get_zone(region):\n",
    "    \"\"\"Fallback zone depuis nom de région.\"\"\"\n",
    "    r = _normalize(region)\n",
    "    for gov, z in GOUVERNORAT_ZONE.items():\n",
    "        if _normalize(gov) in r: return z\n",
    "    for city_norm, (zone, _) in _CITY_NORM.items():\n",
    "        if city_norm in r: return zone\n",
    "    return \"autre\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def extract_rooms(text):\n",
    "    if not text: return None\n",
    "    t = text.lower()\n",
    "    for pat in [r'\\bs\\s*\\+?\\s*(\\d)\\b', r'\\b[ft]\\s*(\\d)\\b',\n",
    "                r'(\\d)\\s*pi[eè]ces?', r'(\\d)\\s*chambres?', r'\\bstudio\\b']:\n",
    "        m = re.search(pat, t)\n",
    "        if m:\n",
    "            if 'studio' in pat: return 1\n",
    "            v = int(m.group(1))\n",
    "            return v if 1 <= v <= 15 else None\n",
    "    return None\n",
    "\n",
    "def extract_surface(text):\n",
    "    if not text: return None\n",
    "    t = text.lower()\n",
    "    for pat in [r'([\\d][\\d\\s.,]*)\\s*m\\s*[²2]',\n",
    "                r'(?:superficie|surface)\\s*(?:de|:)?\\s*([\\d][\\d.,]*)',\n",
    "                r'de\\s+([\\d][\\d.,]*)\\s*m\\s*[²2]']:\n",
    "        m = re.search(pat, t)\n",
    "        if m:\n",
    "            try:\n",
    "                v = float(m.group(1).replace(' ', '').replace(',', '.'))\n",
    "                if 10 <= v <= 50000: return v\n",
    "            except: continue\n",
    "    return None\n",
    "\n",
    "def extract_price(text):\n",
    "    if not text: return None\n",
    "    t = str(text)\n",
    "    t = re.sub(r'(?i)\\b(tnd|dt|dinar|dinars|euro|eur|usd|\\$|€|/mois|/an)\\b', '', t)\n",
    "    t = t.replace('\\xa0', '').replace('\\u202f', '').replace('\\u00a0', '')\n",
    "    digits = re.sub(r'[^\\d.,]', '', t.strip())\n",
    "    if not digits: return None\n",
    "    if re.match(r'^\\d{1,3}[.,]\\d{3}$', digits):\n",
    "        digits = digits.replace('.', '').replace(',', '')\n",
    "    elif digits.count('.') > 1 or digits.count(',') > 1:\n",
    "        digits = digits.replace('.', '').replace(',', '')\n",
    "    elif '.' in digits and ',' in digits:\n",
    "        if digits.index('.') < digits.index(','):\n",
    "            digits = digits.replace('.', '').replace(',', '.')\n",
    "        else:\n",
    "            digits = digits.replace(',', '')\n",
    "    elif ',' in digits and '.' not in digits:\n",
    "        parts = digits.split(',')\n",
    "        if len(parts[1]) == 3:\n",
    "            digits = digits.replace(',', '')\n",
    "        else:\n",
    "            digits = digits.replace(',', '.')\n",
    "    try:\n",
    "        v = float(digits)\n",
    "        return v if 500 <= v <= 100_000_000 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_floor(text):\n",
    "    if not text: return None\n",
    "    t = text.lower()\n",
    "    for pat in [r'(\\d+)[eè]me?\\s*[eé]tage', r'\\b(rdc|rez[- ]de[- ]chauss[eé]e)\\b',\n",
    "                r'\\b(dernier\\s*[eé]tage)\\b', r'[eé]tage\\s*(\\d+)']:\n",
    "        m = re.search(pat, t)\n",
    "        if m: return m.group(0).strip()\n",
    "    return None\n",
    "\n",
    "def extract_poi_from_description(description):\n",
    "    if not description: return {}\n",
    "    poi = {}; t = description.lower()\n",
    "    kws = {\n",
    "        \"école\":        [\"école\",\"ecole\",\"collège\",\"lycée\"],\n",
    "        \"université\":   [\"université\",\"faculté\"],\n",
    "        \"hôpital\":      [\"hôpital\",\"hopital\",\"clinique\",\"polyclinique\"],\n",
    "        \"pharmacie\":    [\"pharmacie\"],\n",
    "        \"supermarché\":  [\"supermarché\",\"superette\",\"carrefour\"],\n",
    "        \"transport\":    [\"bus\",\"métro\",\"train\",\"gare\",\"arrêt\"],\n",
    "        \"mosquée\":      [\"mosquée\",\"mosquee\"],\n",
    "        \"plage\":        [\"plage\",\"mer\"],\n",
    "        \"centre ville\": [\"centre ville\",\"centre-ville\"],\n",
    "        \"aéroport\":     [\"aéroport\",\"aeroport\"],\n",
    "    }\n",
    "    dist_pats = [r'(?:à\\s*)?(\\d+)\\s*m(?:ètres?)?',\n",
    "                 r'(?:à\\s*)?(\\d+(?:[.,]\\d+)?)\\s*km',\n",
    "                 r'(?:à\\s*)?(\\d+)\\s*min(?:utes?)?',\n",
    "                 r'\\b(proche|à proximit[eé]|en face)']\n",
    "    for name, words in kws.items():\n",
    "        for kw in words:\n",
    "            i = t.find(kw)\n",
    "            if i == -1: continue\n",
    "            ctx = t[max(0, i-20):i+60]; dist = None\n",
    "            for dp in dist_pats:\n",
    "                dm = re.search(dp, ctx)\n",
    "                if dm: dist = dm.group(0).strip(); break\n",
    "            poi[name] = dist if dist else \"mentionné\"; break\n",
    "    return poi\n",
    "\n",
    "# ── FILTRE IMAGES — PILLOW UNIQUEMENT ───────────────────────────────────────\n",
    "# Philosophie : NE PAS filtrer par URL, domaine ou extension.\n",
    "# fi-dari.tn sert les logos agences depuis son propre CDN (/static/media/)\n",
    "# avec des URLs sans extension lisible (ex: logo-biat.2781cea3.webp).\n",
    "# La SEULE distinction fiable : les dimensions de l'image.\n",
    "#   - Logo agence     : carré ~200×200 px  → rejeté\n",
    "#   - Photo de bien   : paysage ≥ 400×300 px → conservé\n",
    "#\n",
    "# Seules exclusions URL conservées :\n",
    "#   - data: URLs (base64 inline)\n",
    "#   - .svg / .gif / .ico  (jamais des photos)\n",
    "\n",
    "# Extensions toujours rejetées (vecteurs/animations, jamais des photos réelles)\n",
    "_SKIP_EXT = {\".svg\", \".gif\", \".ico\", \".bmp\", \".tiff\", \".woff\", \".woff2\",\n",
    "             \".ttf\", \".eot\", \".js\", \".css\", \".json\", \".xml\", \".txt\"}\n",
    "\n",
    "def _skip_url(url: str) -> bool:\n",
    "    \"\"\"Rejette uniquement data: et extensions non-image.\"\"\"\n",
    "    if not url or url.startswith(\"data:\"): return True\n",
    "    u = url.lower().split(\"?\")[0].split(\"#\")[0]\n",
    "    # Extensions JS/CSS/font\n",
    "    if any(u.endswith(e) for e in _SKIP_EXT): return True\n",
    "    return False\n",
    "\n",
    "def _normalize_url(src: str) -> str:\n",
    "    \"\"\"Normalise une URL relative en absolue.\"\"\"\n",
    "    src = src.strip().strip(\"'\").strip('\"') \n",
    "    if src.startswith(\"//\"): return \"https:\" + src\n",
    "    if src.startswith(\"/\"): return \"https://fi-dari.tn\" + src\n",
    "    if not src.startswith(\"http\"): return \"https://fi-dari.tn/\" + src.lstrip(\"/\")\n",
    "    return src\n",
    "\n",
    "def _get_hd_url(url: str) -> str:\n",
    "    \"\"\"Essaie de récupérer la version HD en supprimant les suffixes miniature.\"\"\"\n",
    "    return re.sub(r'[_-](thumb|small|medium|xs|sm|md|150x150|300x300|640x|400x)',\n",
    "                  '', url, flags=re.IGNORECASE)\n",
    "\n",
    "def download_all_images(img_urls, prop_type, listing_id):\n",
    "    \"\"\"\n",
    "    Télécharge toutes les images candidates et garde UNIQUEMENT\n",
    "    celles dont Pillow confirme qu'elles sont assez grandes.\n",
    "\n",
    "    SEUL FILTRE : dimensions ≥ MIN_W × MIN_H pixels.\n",
    "      - Logo agence MS Immobilier : ~200×200  → rejeté\n",
    "      - Photo de bien             : ≥ 400×300 → conservé\n",
    "\n",
    "    Pas de filtre URL, pas de filtre domaine, pas de filtre extension.\n",
    "    \"\"\"\n",
    "    MIN_W, MIN_H = 400, 300   # seuil : une vraie photo de bien est au moins 400×300\n",
    "\n",
    "    folder = os.path.join(MEDIA_ROOT, prop_type, str(listing_id))\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    if not img_urls: return None, folder, 0\n",
    "\n",
    "    # Dédoublonner\n",
    "    seen_urls = set()\n",
    "    candidates = []\n",
    "    for url in img_urls:\n",
    "        url = _normalize_url(url)\n",
    "        if url not in seen_urls:\n",
    "            seen_urls.add(url)\n",
    "            candidates.append(url)\n",
    "\n",
    "    saved = []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://fi-dari.tn/\",\n",
    "        \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    # Charger Pillow\n",
    "    try:\n",
    "        from PIL import Image as _PIL\n",
    "        import io as _io\n",
    "        has_pil = True\n",
    "    except ImportError:\n",
    "        has_pil = False\n",
    "        print(\"            ⚠️  Pillow absent — filtre dimensions désactivé (pip install Pillow)\")\n",
    "\n",
    "    for url in candidates:\n",
    "        if _skip_url(url): continue\n",
    "\n",
    "        # Essayer d'abord la version HD\n",
    "        url_hd = _get_hd_url(url)\n",
    "        try_urls = [url_hd, url] if url_hd != url else [url]\n",
    "\n",
    "        for try_url in try_urls:\n",
    "            try:\n",
    "                r = requests.get(try_url, headers=headers, timeout=15, stream=False)\n",
    "                if r.status_code != 200: continue\n",
    "                content = r.content\n",
    "                if len(content) < 1000: continue   # trop petit pour être une image réelle\n",
    "\n",
    "                if has_pil:\n",
    "                    try:\n",
    "                        img_obj = _PIL.open(_io.BytesIO(content))\n",
    "                        w, h = img_obj.size\n",
    "                        if w < MIN_W or h < MIN_H:\n",
    "                            print(f\"            ⏭  {w}×{h}px trop petit → {try_url[-55:]}\")\n",
    "                            break   # rejeter cette URL (pas la peine d'essayer l'original)\n",
    "                    except Exception:\n",
    "                        pass      # Si Pillow ne peut pas lire → garder quand même\n",
    "                else:\n",
    "                    # Sans Pillow : filtrer uniquement par taille fichier (logo ~6-15 KB)\n",
    "                    if len(content) < 30_000:\n",
    "                        print(f\"            ⏭  {len(content)//1024}KB trop petit → {try_url[-55:]}\")\n",
    "                        break\n",
    "\n",
    "                # Déterminer l'extension depuis Content-Type ou URL\n",
    "                ct = r.headers.get(\"Content-Type\", \"\")\n",
    "                if \"webp\" in ct:   ext = \"webp\"\n",
    "                elif \"png\"  in ct: ext = \"png\"\n",
    "                elif \"jpeg\" in ct or \"jpg\" in ct: ext = \"jpg\"\n",
    "                else:\n",
    "                    tail = try_url.split(\"?\")[0].split(\".\")[-1].lower()\n",
    "                    ext  = tail if tail in (\"jpg\",\"jpeg\",\"png\",\"webp\") else \"jpg\"\n",
    "\n",
    "                num = len(saved) + 1\n",
    "                fp  = os.path.join(folder, f\"image_{num:03d}.{ext}\")\n",
    "                with open(fp, \"wb\") as fh:\n",
    "                    fh.write(content)\n",
    "                saved.append(fp)\n",
    "                sz = len(content) // 1024\n",
    "                print(f\"            ✅ image_{num:03d}.{ext}  {w if has_pil else '??'}×{h if has_pil else '??'}px  {sz}KB\")\n",
    "                time.sleep(0.3)\n",
    "                break   # succès → passer à l'image suivante\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(f\"            ❌ {ex} — {try_url[-60:]}\")\n",
    "                continue\n",
    "\n",
    "    if not saved:\n",
    "        print(f\"            ⚠️  Aucune photo conservée pour listing {listing_id}\")\n",
    "    return (saved[0] if saved else None), folder, len(saved)\n",
    "\n",
    "# ── FEATURES ────────────────────────────────────────────────────────────────\n",
    "NAV_EXCLUDE = {\n",
    "    \"blog\",\"accueil\",\"youtube\",\"facebook\",\"instagram\",\"tiktok\",\"twitter\",\n",
    "    \"linkedin\",\"pinterest\",\"se connecter\",\"contactez-nous\",\"contact\",\n",
    "    \"fi-dari.tn\",\"fi-dari\",\"fidari\",\"voir prix\",\"immobilier neuf\",\n",
    "    \"immobilier ancien\",\"terrain\",\"bureau\",\"maison\",\"appartement\",\n",
    "    \"vendre\",\"louer\",\"acheter\",\"recherche\",\"résultats\",\"page\",\"suivant\",\n",
    "    \"précédent\",\"retour\",\"partager\",\"imprimer\",\"signaler\",\"biatimmo\",\n",
    "    \"agence\",\"promoteur\",\"annonceur\",\"newsletter\",\"abonnez\",\"inscription\",\n",
    "    \"connexion\",\"créer un compte\",\"mot de passe\",\"mes annonces\",\"mes favoris\",\n",
    "}\n",
    "PRICE_IN_TEXT = re.compile(r'\\d[\\d\\s]*(?:000)?\\s*(?:dt|tnd|dinar|€|\\$)', re.IGNORECASE)\n",
    "\n",
    "def map_features(raw_list, description=\"\"):\n",
    "    result = {}\n",
    "    for item in raw_list:\n",
    "        item_clean = item.strip()\n",
    "        if not item_clean or len(item_clean) > 120: continue\n",
    "        item_lower = item_clean.lower()\n",
    "        if any(ex in item_lower for ex in NAV_EXCLUDE): continue\n",
    "        if PRICE_IN_TEXT.search(item_clean): continue\n",
    "        if \"http\" in item_lower or \"www.\" in item_lower or \".tn\" in item_lower: continue\n",
    "        if re.match(r'^\\d+$', item_clean): continue\n",
    "        mapped = False\n",
    "        for fr, en in FEATURE_MAPPING.items():\n",
    "            if fr.lower() in item_lower:\n",
    "                result[en] = True; mapped = True; break\n",
    "        if not mapped:\n",
    "            result[item_clean] = True\n",
    "    floor = extract_floor(\" \".join(raw_list) + \" \" + description)\n",
    "    if floor: result[\"floor\"] = floor\n",
    "    return result\n",
    "\n",
    "print(\"✅ Cellule 4 — Utilitaires chargés\")\n",
    "print(f\"   Géolocalisation intelligente : {len(_CITY_NORM)} villes indexées\")\n",
    "print(\"   Filtre images : URL + extension + taille + dimensions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e679e848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cellule 5 — Selenium chargé\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 5 — SELENIUM + SCRAPING\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "_RE_PRICE   = re.compile(r'price|prix|montant|tarif|cost',    re.IGNORECASE)\n",
    "_RE_LOC     = re.compile(r'location|address|region|ville|localisation|adresse|local|lieu|city|where', re.IGNORECASE)\n",
    "_RE_DATE    = re.compile(r'date|update|publication|posted|publi|ajout|modifi', re.IGNORECASE)\n",
    "_RE_DESC    = re.compile(r'description|detail|content|text|body|info|propos',  re.IGNORECASE)\n",
    "_RE_FEAT    = re.compile(r'feature|spec|equipement|caracteristique|attribute|tag|badge|chip|atout|avantage|prive|option', re.IGNORECASE)\n",
    "_RE_GALLERY = re.compile(r'gallery|slider|carousel|photo|image|swiper|lightbox|media|galerie', re.IGNORECASE)\n",
    "_RE_PRICE_TEXT = re.compile(\n",
    "    r'(?:à\\s+partir\\s+de\\s+)?([\\d][\\d\\s.,]*)[\\s\\u00a0\\u202f]*(?:dt|tnd|dinar)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def create_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless: opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    opts.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    opts.set_capability(\"goog:loggingPrefs\", {\"performance\": \"ALL\"})\n",
    "    d = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    d.execute_script(\"Object.defineProperty(navigator,'webdriver',{get:()=>undefined})\")\n",
    "    d.set_page_load_timeout(30)\n",
    "    return d\n",
    "\n",
    "def restart_driver(driver, headless=True):\n",
    "    try: driver.quit()\n",
    "    except: pass\n",
    "    print(\"      🔄 Redémarrage Chrome...\")\n",
    "    time.sleep(5)\n",
    "    return create_driver(headless=headless)\n",
    "\n",
    "def build_url(slug, page_num):\n",
    "    base = re.sub(r'&page=\\d+', '', f\"{BASE_URL}/{slug}\")\n",
    "    return f\"{base}&page={page_num}\"\n",
    "\n",
    "def deep_scroll_and_wait(driver, wait_sec=10):\n",
    "    time.sleep(wait_sec)\n",
    "    total = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    step  = max(200, total // 15); pos = 0\n",
    "    while pos < total:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {pos});\")\n",
    "        time.sleep(0.3); pos += step\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h > total: total = new_h\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "def get_network_requests(driver):\n",
    "    api_data = []\n",
    "    for entry in driver.get_log(\"performance\"):\n",
    "        try:\n",
    "            msg = json.loads(entry[\"message\"])[\"message\"]\n",
    "            if msg[\"method\"] != \"Network.responseReceived\": continue\n",
    "            resp = msg[\"params\"].get(\"response\", {})\n",
    "            url  = resp.get(\"url\", \"\"); mime = resp.get(\"mimeType\", \"\")\n",
    "            if \"json\" in mime and any(k in url.lower() for k in\n",
    "                    [\"annonce\",\"search\",\"listing\",\"bien\",\"property\",\"immobil\",\"api\"]):\n",
    "                api_data.append({\"url\": url, \"requestId\": msg[\"params\"].get(\"requestId\", \"\")})\n",
    "        except: continue\n",
    "    return api_data\n",
    "\n",
    "def fetch_request_body(driver, request_id):\n",
    "    try:\n",
    "        r    = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "        body = r.get(\"body\", \"\")\n",
    "        if r.get(\"base64Encoded\", False):\n",
    "            import base64\n",
    "            body = base64.b64decode(body).decode(\"utf-8\", errors=\"ignore\")\n",
    "        return json.loads(body)\n",
    "    except: return None\n",
    "\n",
    "def get_annonces_from_list_page(driver, url):\n",
    "    print(f\"      🌐 Chargement...\")\n",
    "    driver.get(url)\n",
    "    deep_scroll_and_wait(driver, wait_sec=10)\n",
    "\n",
    "    for call in get_network_requests(driver):\n",
    "        data = fetch_request_body(driver, call[\"requestId\"])\n",
    "        if data:\n",
    "            ann = extract_from_api_response(data)\n",
    "            if ann: print(f\"      ✅ API JSON : {len(ann)} annonces\"); return ann\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    urls = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if any(p in href for p in [\"/bien/\", \"/annonce/\", \"/detail/\"]):\n",
    "            if not href.startswith(\"http\"): href = BASE_URL + href\n",
    "            urls.add(href)\n",
    "    if urls:\n",
    "        print(f\"      ✅ Liens HTML : {len(urls)} annonces\")\n",
    "        return [{\"url\": u, \"title_hint\": \"\", \"price_hint\": \"\", \"location_hint\": \"\", \"thumb\": None}\n",
    "                for u in urls]\n",
    "\n",
    "    items = find_listing_items(soup)\n",
    "    result, seen = [], set()\n",
    "    for item in items:\n",
    "        a = item.find(\"a\", href=True)\n",
    "        if not a: continue\n",
    "        href = a[\"href\"]\n",
    "        if not href.startswith(\"http\"): href = BASE_URL + href\n",
    "        if href in seen: continue\n",
    "        seen.add(href); result.append(extract_card_data(item, href))\n",
    "    if result:\n",
    "        print(f\"      ✅ CSS heuristique : {len(result)} annonces\")\n",
    "        return result\n",
    "\n",
    "    print(f\"      ❌ Aucune annonce\")\n",
    "    with open(f\"debug_{int(time.time())}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(driver.page_source)\n",
    "    return []\n",
    "\n",
    "def extract_from_api_response(data):\n",
    "    annonces = []\n",
    "    def find_list(obj, depth=0):\n",
    "        if depth > 5: return []\n",
    "        if isinstance(obj, list) and len(obj) > 1:\n",
    "            s = obj[0]\n",
    "            if isinstance(s, dict) and any(k in s for k in\n",
    "                    [\"url\",\"price\",\"prix\",\"titre\",\"title\",\"reference\",\"ref\"]): return obj\n",
    "        if isinstance(obj, dict):\n",
    "            for v in obj.values():\n",
    "                r = find_list(v, depth + 1)\n",
    "                if r: return r\n",
    "        return []\n",
    "    for item in find_list(data):\n",
    "        if not isinstance(item, dict): continue\n",
    "        url = (item.get(\"url\") or item.get(\"link\") or item.get(\"slug\") or \"\")\n",
    "        if url and not url.startswith(\"http\"): url = BASE_URL + \"/bien/\" + str(url)\n",
    "        if not url: continue\n",
    "        loc_hint = str(item.get(\"localisation\",\"\") or item.get(\"location\",\"\") or\n",
    "                       item.get(\"ville\",\"\") or item.get(\"gouvernorat\",\"\") or \"\")\n",
    "        annonces.append({\n",
    "            \"url\":           url,\n",
    "            \"title_hint\":    str(item.get(\"titre\",\"\") or item.get(\"title\",\"\") or \"\"),\n",
    "            \"price_hint\":    str(item.get(\"prix\",\"\")  or item.get(\"price\",\"\") or \"\"),\n",
    "            \"location_hint\": loc_hint,\n",
    "            \"thumb\":         item.get(\"photo\") or item.get(\"image\"),\n",
    "            \"raw_data\":      item,\n",
    "        })\n",
    "    return annonces\n",
    "\n",
    "def find_listing_items(soup):\n",
    "    cc = Counter()\n",
    "    for tag in soup.find_all([\"div\",\"article\",\"li\",\"a\"]):\n",
    "        for cls in tag.get(\"class\",[]): cc[cls] += 1\n",
    "    best = []\n",
    "    for cls, cnt in cc.items():\n",
    "        if not (4 <= cnt <= 100): continue\n",
    "        items = soup.find_all(attrs={\"class\": re.compile(re.escape(cls))})\n",
    "        valid = [i for i in items if i.find(\"img\") and i.find(\"a\",href=True)\n",
    "                 and len(i.get_text(strip=True)) > 20]\n",
    "        if len(valid) > len(best): best = valid\n",
    "    return best\n",
    "\n",
    "def extract_card_data(item, url):\n",
    "    title = \"\"\n",
    "    for sel in [\"h1\",\"h2\",\"h3\",\"h4\"]:\n",
    "        el = item.find(sel)\n",
    "        if el:\n",
    "            t = el.get_text(strip=True)\n",
    "            if 5 < len(t) < 200: title = t; break\n",
    "    price_hint = \"\"\n",
    "    for el in item.find_all(class_=_RE_PRICE):\n",
    "        t = el.get_text(strip=True).replace(\"\\xa0\",\"\").replace(\"\\u202f\",\"\")\n",
    "        if any(c.isdigit() for c in t): price_hint = t; break\n",
    "    loc = \"\"\n",
    "    for el in item.find_all(class_=_RE_LOC):\n",
    "        t = el.get_text(strip=True)\n",
    "        if 2 < len(t) < 150: loc = t; break\n",
    "    thumb = None\n",
    "    img = item.find(\"img\")\n",
    "    if img:\n",
    "        thumb = (img.get(\"data-src\") or img.get(\"data-lazy\") or\n",
    "                 img.get(\"data-original\") or img.get(\"src\",\"\"))\n",
    "        if thumb and not thumb.startswith(\"http\"):\n",
    "            thumb = BASE_URL + \"/\" + thumb.lstrip(\"/\")\n",
    "        if thumb and _is_parasite_url(thumb): thumb = None\n",
    "    return {\"url\":url,\"title_hint\":title,\"price_hint\":price_hint,\n",
    "            \"location_hint\":loc,\"thumb\":thumb}\n",
    "\n",
    "def extract_price_fidari(soup):\n",
    "    for tag in soup.find_all(class_=_RE_PRICE):\n",
    "        t = tag.get_text(strip=True).replace(\"\\xa0\",\"\").replace(\"\\u202f\",\"\")\n",
    "        v = extract_price(t)\n",
    "        if v: return v\n",
    "    for meta in soup.find_all(\"meta\"):\n",
    "        prop = (meta.get(\"property\",\"\") + meta.get(\"name\",\"\")).lower()\n",
    "        if \"price\" in prop or \"prix\" in prop:\n",
    "            v = extract_price(meta.get(\"content\",\"\"))\n",
    "            if v: return v\n",
    "    m = _RE_PRICE_TEXT.search(soup.get_text(\" \"))\n",
    "    if m:\n",
    "        v = extract_price(m.group(1))\n",
    "        if v: return v\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        v = extract_price(h1.get_text())\n",
    "        if v: return v\n",
    "    return None\n",
    "\n",
    "def extract_date_fidari(soup):\n",
    "    for tag in soup.find_all(\"time\"):\n",
    "        dt = tag.get(\"datetime\",\"\")\n",
    "        if dt: return dt\n",
    "        t = tag.get_text(strip=True)\n",
    "        if len(t) > 3: return t\n",
    "    for tag in soup.find_all(class_=_RE_DATE):\n",
    "        t = tag.get_text(strip=True)\n",
    "        if 3 < len(t) < 80: return t\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    for pat in [\n",
    "        r'(?:modifi[eé]e?|publi[eé]e?|ajout[eé]e?|mise?\\s+à\\s+jour)\\s*(?:le\\s*)?(\\d{1,2}[\\s/\\-\\.]\\w+[\\s/\\-\\.]\\d{2,4})',\n",
    "        r'(il\\s+y\\s+a\\s+\\d+\\s*(?:jour|heure|minute|mois|an)s?)',\n",
    "        r'\\b(\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{4})\\b',\n",
    "        r'\\b(\\d{1,2}\\s+(?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\s+\\d{4})\\b',\n",
    "    ]:\n",
    "        m = re.search(pat, text, re.IGNORECASE)\n",
    "        if m: return m.group(1).strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_pdf_fidari(soup):\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        text = a.get_text(strip=True).lower()\n",
    "        href_low = href.lower()\n",
    "        if href_low.endswith(\".pdf\"):\n",
    "            return href if href.startswith(\"http\") else BASE_URL + \"/\" + href.lstrip(\"/\")\n",
    "        if any(kw in text for kw in [\"fiche\",\"télécharger\",\"download\",\"pdf\",\"brochure\"]):\n",
    "            return href if href.startswith(\"http\") else BASE_URL + \"/\" + href.lstrip(\"/\")\n",
    "        if any(kw in href_low for kw in [\".pdf\",\"fiche\",\"download\",\"telecharger\",\"brochure\"]):\n",
    "            return href if href.startswith(\"http\") else BASE_URL + \"/\" + href.lstrip(\"/\")\n",
    "    return None\n",
    "\n",
    "def extract_location_from_page(soup, driver_url=\"\"):\n",
    "    \"\"\"\n",
    "    Cherche la localisation dans les balises HTML de la page de détail.\n",
    "    Retourne le texte brut le plus pertinent.\n",
    "    \"\"\"\n",
    "    # 1. Breadcrumb / fil d'Ariane\n",
    "    for tag in soup.find_all([\"nav\",\"ol\",\"ul\"], class_=re.compile(r'breadcrumb|fil|ariane|path', re.I)):\n",
    "        items = tag.find_all([\"li\",\"a\",\"span\"])\n",
    "        parts = [i.get_text(strip=True) for i in items if i.get_text(strip=True)]\n",
    "        if len(parts) >= 2:\n",
    "            # Exclure le premier (Accueil) et le dernier (titre annonce)\n",
    "            geo_parts = [p for p in parts[1:-1] if len(p) > 2 and len(p) < 60]\n",
    "            if geo_parts: return \" - \".join(geo_parts)\n",
    "    # 2. Span/div localisation\n",
    "    for tag in soup.find_all(class_=_RE_LOC):\n",
    "        t = tag.get_text(\" \", strip=True)\n",
    "        if 3 < len(t) < 150: return t\n",
    "    # 3. Icône localisation (souvent un i ou svg suivi du texte)\n",
    "    for tag in soup.find_all([\"p\",\"span\",\"div\"]):\n",
    "        text = tag.get_text(\" \", strip=True)\n",
    "        if any(kw in text.lower() for kw in [\"tunisie\",\"tunis\",\"sfax\",\"sousse\",\"nabeul\",\n",
    "                \"monastir\",\"bizerte\",\"gabes\",\"kairouan\",\"sousse\",\"mahdia\"]):\n",
    "            if 3 < len(text) < 150: return text\n",
    "    # 4. URL : extraire depuis le slug /bien/ville-...\n",
    "    m = re.search(r'/bien/([^/?#]+)', driver_url)\n",
    "    if m:\n",
    "        slug = m.group(1).replace(\"-\", \" \")\n",
    "        return slug\n",
    "    return \"\"\n",
    "\n",
    "def collect_image_urls(soup, driver):\n",
    "    \"\"\"\n",
    "    Collecte TOUTES les URLs d'images de la page sans filtrage URL.\n",
    "    Le filtrage se fait UNIQUEMENT dans download_all_images() par dimensions Pillow.\n",
    "\n",
    "    Sources (par ordre de fiabilité) :\n",
    "    1. Log réseau Chrome  → toutes les images réellement chargées par le navigateur\n",
    "    2. Attributs <img>    → data-big / data-original / data-src / src\n",
    "    3. Background CSS     → url(...) dans style=\"\"\n",
    "    \"\"\"\n",
    "    seen   = set()\n",
    "    result = []\n",
    "\n",
    "    def add(src):\n",
    "        if not src: return\n",
    "        src = src.strip().strip(\"'\").strip('\"') \n",
    "        if not src or src.startswith(\"data:\"): return\n",
    "        if src.startswith(\"//\"): src = \"https:\" + src\n",
    "        if src.startswith(\"/\"): src = \"https://fi-dari.tn\" + src\n",
    "        if not src.startswith(\"http\"): src = \"https://fi-dari.tn/\" + src.lstrip(\"/\")\n",
    "        # Exclure uniquement les ressources non-image évidentes\n",
    "        u = src.lower().split(\"?\")[0]\n",
    "        if any(u.endswith(e) for e in (\".svg\",\".gif\",\".ico\",\".js\",\".css\",\n",
    "                                        \".woff\",\".woff2\",\".ttf\",\".json\")): return\n",
    "        if src not in seen:\n",
    "            seen.add(src)\n",
    "            result.append(src)\n",
    "\n",
    "    # ── 1. Log réseau Chrome ─────────────────────────────────\n",
    "    # React charge les images via XHR/fetch → elles apparaissent dans les logs\n",
    "    # C'est la source la plus fiable : on voit exactement ce qui s'affiche\n",
    "    net_count = 0\n",
    "    for entry in driver.get_log(\"performance\"):\n",
    "        try:\n",
    "            msg = json.loads(entry[\"message\"])[\"message\"]\n",
    "            # Requêtes réseau sortantes\n",
    "            if msg[\"method\"] == \"Network.requestWillBeSent\":\n",
    "                req_url = msg[\"params\"][\"request\"][\"url\"]\n",
    "                u_low = req_url.lower()\n",
    "                # Garder uniquement les ressources image (par Content-Type ou extension)\n",
    "                if any(e in u_low for e in (\".jpg\",\".jpeg\",\".png\",\".webp\",\n",
    "                                             \"image/\",\"media/\",\"upload\",\"static\")):\n",
    "                    add(req_url)\n",
    "                    net_count += 1\n",
    "        except: continue\n",
    "\n",
    "    # ── 2. Attributs <img> ───────────────────────────────────\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        for attr in [\"data-big\",\"data-original\",\"data-full\",\n",
    "                     \"data-src\",\"data-lazy\",\"data-zoom-image\",\"src\"]:\n",
    "            val = img.get(attr, \"\")\n",
    "            if val: add(val); break\n",
    "\n",
    "    # ── 3. Background CSS ────────────────────────────────────\n",
    "    for el in soup.find_all(style=True):\n",
    "        for m in re.finditer(r\"url\\(([^)]+)\\)\", el.get(\"style\", \"\")):\n",
    "            src_css = m.group(1).strip().strip(\"'\").strip('\"')\n",
    "            add(src_css)\n",
    "\n",
    "    print(f\"            📦 {len(result)} URLs candidates ({net_count} réseau, \"\n",
    "          f\"{len(result)-net_count} HTML)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def scrape_detail_page(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        deep_scroll_and_wait(driver, wait_sec=7)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        data = {}\n",
    "\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1 and len(h1.get_text(strip=True)) > 5:\n",
    "            data[\"title_detail\"] = h1.get_text(strip=True)\n",
    "\n",
    "        desc = \"\"; best_len = 0\n",
    "        for tag in soup.find_all([\"div\",\"p\",\"section\"]):\n",
    "            cs = \" \".join(tag.get(\"class\",[]))\n",
    "            if _RE_DESC.search(cs):\n",
    "                t = tag.get_text(\" \", strip=True)\n",
    "                if 30 < len(t) < 5000 and len(t) > best_len: best_len = len(t); desc = t\n",
    "        data[\"description\"] = desc\n",
    "\n",
    "        data[\"price\"]        = extract_price_fidari(soup)\n",
    "        data[\"last_updated\"] = extract_date_fidari(soup)\n",
    "        data[\"pdf_link\"]     = extract_pdf_fidari(soup)\n",
    "\n",
    "        # Localisation enrichie\n",
    "        loc_raw = extract_location_from_page(soup, driver_url=url)\n",
    "        data[\"location_raw\"] = loc_raw\n",
    "\n",
    "        raw_feats = []\n",
    "        for tag in soup.find_all(class_=_RE_FEAT):\n",
    "            t = tag.get_text(strip=True)\n",
    "            if 2 < len(t) < 100: raw_feats.append(t)\n",
    "        for ul in soup.find_all(\"ul\"):\n",
    "            lis = ul.find_all(\"li\")\n",
    "            if 2 <= len(lis) <= 30:\n",
    "                for li in lis:\n",
    "                    t = li.get_text(strip=True)\n",
    "                    if 2 < len(t) < 80: raw_feats.append(t)\n",
    "        raw_feats = list(dict.fromkeys(raw_feats))\n",
    "        specs     = \" \".join(raw_feats)\n",
    "\n",
    "        data[\"surface_detail\"] = extract_surface(specs) or extract_surface(desc)\n",
    "        data[\"rooms_detail\"]   = extract_rooms(specs)   or extract_rooms(desc)\n",
    "        data[\"features\"]       = map_features(raw_feats, desc)\n",
    "        data[\"poi\"]            = extract_poi_from_description(desc)\n",
    "        data[\"img_urls\"]       = collect_image_urls(soup, driver)\n",
    "        data[\"_soup\"]          = soup  # transmis pour build_location\n",
    "\n",
    "        print(f\"         🖼️  {len(data['img_urls'])} photos | 💰 {data['price']} | 📅 {data['last_updated']} | 📍 {loc_raw[:40]}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"      ⚠️ scrape_detail: {e}\")\n",
    "        return {}\n",
    "\n",
    "print(\"✅ Cellule 5 — Selenium chargé\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c07c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC fi-dari.tn\n",
      "\n",
      "   ⏳ Attente React (15s)...\n",
      "   📑 Titre : Rechercher un bien immobilier en Tunisie\n",
      "   📏 HTML  : 47,479 chars\n",
      "\n",
      "   🔗 Annonces trouvées : 10\n",
      "      → /bien/Yesmine hammamet-PA32031-1\n",
      "      → /bien/Hammemet centre-PA32022-1\n",
      "      → /bien/Hammemet centre-PA32018-1\n",
      "      → /bien/Hammemet centre-PA31994-1\n",
      "      → /bien/Hammemet centre-PA31992-1\n",
      "\n",
      "   🧪 Test détail : https://fi-dari.tn/bien/Yesmine hammamet-PA32031-1\n",
      "            📦 47 URLs candidates (64 réseau, -17 HTML)\n",
      "      💰 Prix       : 360000.0\n",
      "      📅 Date       : 19 février 2026\n",
      "      📄 PDF        : None\n",
      "      📍 Loc brute  : Achat appartement s+1 de 90m² Yesmine hammamet, Nabeul Simul\n",
      "      🗺️  Region     : Nabeul\n",
      "      🏙️  Municipalité: Hammamet\n",
      "      🌍 Zone       : nord\n",
      "      🖼️  Images     : 47\n",
      "         https://fi-dari.tn/static/media/logo-fidari.938f7b7a.webp\n",
      "         https://b.tile.openstreetmap.fr/osmfr/6/33/25.png\n",
      "         https://c.tile.openstreetmap.fr/osmfr/6/34/25.png\n",
      "\n",
      "   💾 fidari_diagnostic.html sauvegardé\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "   ⏸️  Appuie ENTRÉE pour fermer Chrome... \n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 6 — DIAGNOSTIC VISUEL\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def run_diagnostic():\n",
    "    test_url = (f\"{BASE_URL}/search?objectif=vendre\"\n",
    "                f\"&categorie=Appartement&usage=Habitation&bounds={B}&page=1\")\n",
    "    print(\"🔍 DIAGNOSTIC fi-dari.tn\\n\")\n",
    "    d = create_driver(headless=False)\n",
    "    try:\n",
    "        d.get(test_url)\n",
    "        print(\"   ⏳ Attente React (15s)...\")\n",
    "        deep_scroll_and_wait(d, wait_sec=12)\n",
    "        soup = BeautifulSoup(d.page_source, \"html.parser\")\n",
    "        print(f\"   📑 Titre : {soup.title.string if soup.title else 'N/A'}\")\n",
    "        print(f\"   📏 HTML  : {len(d.page_source):,} chars\")\n",
    "        liens = [a[\"href\"] for a in soup.find_all(\"a\", href=True)\n",
    "                 if any(p in a[\"href\"] for p in [\"/bien/\",\"/annonce/\",\"/detail/\"])]\n",
    "        print(f\"\\n   🔗 Annonces trouvées : {len(liens)}\")\n",
    "        for l in liens[:5]: print(f\"      → {l[:90]}\")\n",
    "        if liens:\n",
    "            test_detail = liens[0] if liens[0].startswith(\"http\") else BASE_URL + liens[0]\n",
    "            print(f\"\\n   🧪 Test détail : {test_detail}\")\n",
    "            d.get(test_detail)\n",
    "            deep_scroll_and_wait(d, wait_sec=8)\n",
    "            soup2 = BeautifulSoup(d.page_source, \"html.parser\")\n",
    "            loc_raw = extract_location_from_page(soup2, driver_url=test_detail)\n",
    "            geo     = build_location(loc_raw,\n",
    "                                     title=soup2.find(\"h1\").get_text() if soup2.find(\"h1\") else \"\",\n",
    "                                     soup=soup2)\n",
    "            imgs    = collect_image_urls(soup2, d)\n",
    "            print(f\"      💰 Prix       : {extract_price_fidari(soup2)}\")\n",
    "            print(f\"      📅 Date       : {extract_date_fidari(soup2)}\")\n",
    "            print(f\"      📄 PDF        : {extract_pdf_fidari(soup2)}\")\n",
    "            print(f\"      📍 Loc brute  : {loc_raw[:60]}\")\n",
    "            print(f\"      🗺️  Region     : {geo['region']}\")\n",
    "            print(f\"      🏙️  Municipalité: {geo['municipality']}\")\n",
    "            print(f\"      🌍 Zone       : {geo['zone']}\")\n",
    "            print(f\"      🖼️  Images     : {len(imgs)}\")\n",
    "            for u in imgs[:3]: print(f\"         {u[:90]}\")\n",
    "        with open(\"fidari_diagnostic.html\",\"w\",encoding=\"utf-8\") as f:\n",
    "            f.write(d.page_source)\n",
    "        print(\"\\n   💾 fidari_diagnostic.html sauvegardé\")\n",
    "    finally:\n",
    "        input(\"\\n   ⏸️  Appuie ENTRÉE pour fermer Chrome...\")\n",
    "        d.quit()\n",
    "\n",
    "run_diagnostic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SCRAPING INITIAL Fi-Dari\n",
      "   Objectif  : 200 par catégorie\n",
      "   Total visé: 1600 annonces\n",
      "\n",
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  📦  APARTMENT | SALE\n",
      "═════════════════════════════════════════════════════════════════\n",
      "\n",
      "  📄 Page 1\n",
      "      🌐 Chargement...\n",
      "      ✅ Liens HTML : 10 annonces\n",
      "      📋 10 annonces\n",
      "      ⬇️  [1/200] ge=Habitation&bounds=[[37.649,7.778],[30.107,11.953]]&page=1\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 7 — SCRAPING INITIAL (lancer UNE SEULE FOIS)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "conn   = get_conn()\n",
    "cur    = conn.cursor()\n",
    "driver = create_driver(headless=True)\n",
    "\n",
    "log_id       = log_start(cur, \"initial\")\n",
    "conn.commit()\n",
    "total_saved  = 0\n",
    "total_errors = 0\n",
    "stats        = {}\n",
    "\n",
    "print(\"🚀 SCRAPING INITIAL Fi-Dari\")\n",
    "print(f\"   Objectif  : {TARGET_PER_TYPE} par catégorie\")\n",
    "print(f\"   Total visé: {TARGET_PER_TYPE * len(CATEGORIES)} annonces\\n\")\n",
    "\n",
    "try:\n",
    "    for slug, (trans_type, prop_type) in CATEGORIES.items():\n",
    "        cat_key   = f\"{prop_type}_{trans_type}\"\n",
    "        cat_saved = 0\n",
    "        print(f\"\\n{'═'*65}\")\n",
    "        print(f\"  📦  {prop_type.upper()} | {trans_type.upper()}\")\n",
    "        print(f\"{'═'*65}\")\n",
    "\n",
    "        for page_num in range(1, MAX_PAGES_INITIAL + 1):\n",
    "            if cat_saved >= TARGET_PER_TYPE:\n",
    "                print(f\"  🎯 Objectif {TARGET_PER_TYPE} atteint.\"); break\n",
    "\n",
    "            url_page      = build_url(slug, page_num)\n",
    "            print(f\"\\n  📄 Page {page_num}\")\n",
    "            page_annonces = []\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    page_annonces = get_annonces_from_list_page(driver, url_page); break\n",
    "                except Exception as e:\n",
    "                    print(f\"      ⚠️ Retry {attempt+1}/3: {e}\")\n",
    "                    driver = restart_driver(driver); time.sleep(random.uniform(5,10))\n",
    "\n",
    "            if not page_annonces:\n",
    "                print(\"  🛑 Fin de catégorie.\"); break\n",
    "\n",
    "            print(f\"      📋 {len(page_annonces)} annonces\")\n",
    "\n",
    "            for ann in page_annonces:\n",
    "                if cat_saved >= TARGET_PER_TYPE: break\n",
    "                ann_url = ann.get(\"url\",\"\")\n",
    "                if not ann_url or url_exists(cur, ann_url): continue\n",
    "\n",
    "                print(f\"      ⬇️  [{cat_saved+1}/{TARGET_PER_TYPE}] {ann_url[-60:]}\")\n",
    "\n",
    "                det = {}\n",
    "                for attempt in range(3):\n",
    "                    try: det = scrape_detail_page(driver, ann_url); break\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ⚠️ Retry détail {attempt+1}/3: {e}\")\n",
    "                        driver = restart_driver(driver); time.sleep(random.uniform(5,10))\n",
    "\n",
    "                title   = det.get(\"title_detail\") or ann.get(\"title_hint\",\"\") or \"N/A\"\n",
    "                price   = det.get(\"price\") or extract_price(ann.get(\"price_hint\",\"\"))\n",
    "                loc_raw = det.get(\"location_raw\",\"\") or ann.get(\"location_hint\",\"\")\n",
    "                desc    = det.get(\"description\",\"\")\n",
    "                soup_   = det.get(\"_soup\")\n",
    "                cfg     = PROPERTY_CONFIG.get(prop_type, {\"has_rooms\":True,\"has_surface\":True})\n",
    "\n",
    "                # ── Géolocalisation intelligente ──────────────────────────\n",
    "                geo = build_location(\n",
    "                    raw_loc     = loc_raw,\n",
    "                    title       = title,\n",
    "                    description = desc,\n",
    "                    soup        = soup_\n",
    "                )\n",
    "\n",
    "                surface = (det.get(\"surface_detail\") or extract_surface(title) or\n",
    "                           extract_surface(desc)) if cfg[\"has_surface\"] else None\n",
    "                rooms   = (det.get(\"rooms_detail\") or extract_rooms(title) or\n",
    "                           extract_rooms(desc)) if cfg[\"has_rooms\"] else None\n",
    "\n",
    "                data = {\n",
    "                    \"title\": title, \"price\": price,\n",
    "                    \"transaction_type\": trans_type, \"type\": prop_type,\n",
    "                    \"region\": geo[\"region\"], \"municipality\": geo[\"municipality\"],\n",
    "                    \"zone\": geo[\"zone\"], \"location_details\": geo[\"location_details\"],\n",
    "                    \"surface\": surface, \"rooms\": rooms,\n",
    "                    \"features\": det.get(\"features\",{}), \"poi\": det.get(\"poi\",{}),\n",
    "                    \"description\": desc, \"url\": ann_url,\n",
    "                    \"pdf_link\": det.get(\"pdf_link\"),\n",
    "                    \"last_updated\": det.get(\"last_updated\",\"Unknown\"),\n",
    "                }\n",
    "\n",
    "                listing_id = insert_listing(cur, data)\n",
    "                if not listing_id: total_errors += 1; continue\n",
    "                conn.commit()\n",
    "\n",
    "                img_urls = list(det.get(\"img_urls\",[]))\n",
    "                if ann.get(\"thumb\") and ann[\"thumb\"] not in img_urls:\n",
    "                    if not _is_parasite_url(ann[\"thumb\"]) and _is_photo_url(ann[\"thumb\"]):\n",
    "                        img_urls.insert(0, ann[\"thumb\"])\n",
    "\n",
    "                thumb, folder, nb = download_all_images(img_urls, prop_type, listing_id)\n",
    "                update_images(cur, listing_id, thumb, folder, nb)\n",
    "                conn.commit()\n",
    "\n",
    "                cat_saved   += 1\n",
    "                total_saved += 1\n",
    "                print(f\"         ✅ id={listing_id} | {geo['municipality']},{geo['region']} [{geo['zone']}] | prix={price} | {surface}m² | {nb} imgs\")\n",
    "                time.sleep(random.uniform(2,4))\n",
    "\n",
    "            time.sleep(random.uniform(3,6))\n",
    "\n",
    "        stats[cat_key] = cat_saved\n",
    "        print(f\"\\n  📊 {cat_key} → {cat_saved} annonces\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️ Interrompu.\")\n",
    "\n",
    "finally:\n",
    "    log_end(cur, log_id, total_saved, total_errors)\n",
    "    conn.commit()\n",
    "    try: driver.quit()\n",
    "    except: pass\n",
    "    cur.close(); conn.close()\n",
    "    print(f\"\\n{'═'*65}\")\n",
    "    print(f\"  🎉 TERMINÉ — {total_saved} annonces | {total_errors} erreurs\")\n",
    "    print(f\"{'═'*65}\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  {k:<35} : {v:>4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c19af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK B:\\EstateMind\\fi_dari_scraper\\daily_scrape.py\n",
      "OK B:\\EstateMind\\fi_dari_scraper\\lancer_daily.bat\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 8 — GÉNÉRATION daily_scrape.py + lancer_daily.bat\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "import sys as _sys\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = r\"B:\\EstateMind\\fi_dari_scraper\"\n",
    "os.makedirs(os.path.join(PROJECT_DIR, \"logs\"), exist_ok=True)\n",
    "DAILY_PY   = os.path.join(PROJECT_DIR, \"daily_scrape.py\")\n",
    "BAT_FILE   = os.path.join(PROJECT_DIR, \"lancer_daily.bat\")\n",
    "PYTHON_EXE = _sys.executable\n",
    "\n",
    "DAILY_CONTENT = r\"\"\"# daily_scrape.py — Cycle quotidien Fi-Dari\n",
    "import os, re, time, json, random, requests, psycopg2, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "DB_CONFIG={\"host\":\"localhost\",\"database\":\"estate_mind_db\",\"user\":\"postgres\",\"password\":\"admin\"}\n",
    "MEDIA_ROOT=r\"C:\\EstateMind\\media_fidari\"\n",
    "BASE_URL=\"https://fi-dari.tn\"\n",
    "B=\"[[37.649,7.778],[30.107,11.953]]\"\n",
    "MAX_PAGES_DAILY=3\n",
    "\n",
    "CATEGORIES={\n",
    "    f\"search?objectif=vendre&categorie=Appartement&usage=Habitation&bounds={B}\":(\"sale\",\"apartment\"),\n",
    "    f\"search?objectif=louer&categorie=Appartement&usage=Tout+type+de+location&bounds={B}\":(\"rent\",\"apartment\"),\n",
    "    f\"search?objectif=vendre&categorie=Maison&usage=Habitation&bounds={B}\":(\"sale\",\"house\"),\n",
    "    f\"search?objectif=louer&categorie=Maison&usage=Tout+type+de+location&bounds={B}\":(\"rent\",\"house\"),\n",
    "    f\"search?objectif=vendre&categorie=Terrain&bounds={B}\":(\"sale\",\"land\"),\n",
    "    f\"search?objectif=louer&categorie=Terrain&bounds={B}\":(\"rent\",\"land\"),\n",
    "    f\"search?objectif=vendre&categorie=Bureau&usage=Professionnels&bounds={B}\":(\"sale\",\"office\"),\n",
    "    f\"search?objectif=louer&categorie=Bureau&usage=Bureaux+et+commerce&bounds={B}\":(\"rent\",\"office\"),\n",
    "}\n",
    "\n",
    "PROPERTY_CONFIG={\n",
    "    'apartment':{'has_rooms':True,'has_surface':True},\n",
    "    'house':{'has_rooms':True,'has_surface':True},\n",
    "    'land':{'has_rooms':False,'has_surface':True},\n",
    "    'office':{'has_rooms':False,'has_surface':True}\n",
    "}\n",
    "\n",
    "GOUVERNORAT_ZONE={\n",
    "    'Tunis':'grand_tunis','Ariana':'grand_tunis','Ben Arous':'grand_tunis','Manouba':'grand_tunis',\n",
    "    'Bizerte':'nord','Nabeul':'nord','Beja':'nord','Jendouba':'nord','Zaghouan':'nord',\n",
    "    'Sousse':'sahel','Monastir':'sahel','Mahdia':'sahel',\n",
    "    'Kairouan':'centre','Kasserine':'centre','Sidi Bouzid':'centre','Siliana':'centre','Gafsa':'centre',\n",
    "    'Sfax':'sud','Gabes':'sud','Medenine':'sud','Tataouine':'sud','Tozeur':'sud','Kebili':'sud'\n",
    "}\n",
    "\n",
    "FEATURE_MAPPING={\n",
    "    'Climatisation':'has_air_conditioning','Ascenseur':'has_elevator','Jardin':'has_garden',\n",
    "    'Piscine':'has_pool','Terrasse':'has_terrace','Parking':'has_parking',\n",
    "    'Vue sur mer':'has_sea_view','Meuble':'is_furnished','Balcon':'has_balcony',\n",
    "    'Digicode':'has_digicode','Lumineux':'is_bright','Calme':'is_quiet'\n",
    "}\n",
    "\n",
    "_PARASITE_KW=[\n",
    "    'biatimmo','fleximmo','logo','icon','/icons/','picto','sprite','favicon','avatar',\n",
    "    'placeholder','loading','noimage','no-image','banner','ads/','agence','promoteur',\n",
    "    'facebook','instagram','youtube','google','gstatic','map-marker','check','arrow','btn-',\n",
    "    '.svg','.gif','.ico'\n",
    "]\n",
    "_PHOTO_EXT={'.jpg','.jpeg','.png','.webp'}\n",
    "\n",
    "def _norm(s):\n",
    "    return unicodedata.normalize('NFKD',s.lower()).encode('ascii','ignore').decode()\n",
    "\n",
    "CITY_GEO={_norm(k):v for k,v in {\n",
    "    'tunis':('grand_tunis','Tunis'),'ariana':('grand_tunis','Ariana'),\n",
    "    'ben arous':('grand_tunis','Ben Arous'),'manouba':('grand_tunis','Manouba'),\n",
    "    'la marsa':('grand_tunis','Tunis'),'marsa':('grand_tunis','Tunis'),\n",
    "    'bardo':('grand_tunis','Tunis'),'el menzah':('grand_tunis','Ariana'),\n",
    "    'ennasr':('grand_tunis','Ariana'),'soukra':('grand_tunis','Ariana'),\n",
    "    'nabeul':('nord','Nabeul'),'hammamet':('nord','Nabeul'),'bizerte':('nord','Bizerte'),\n",
    "    'jendouba':('nord','Jendouba'),'beja':('nord','Beja'),'zaghouan':('nord','Zaghouan'),\n",
    "    'kelibia':('nord','Nabeul'),'korba':('nord','Nabeul'),\n",
    "    'sousse':('sahel','Sousse'),'monastir':('sahel','Monastir'),'mahdia':('sahel','Mahdia'),\n",
    "    'msaken':('sahel','Sousse'),'kantaoui':('sahel','Sousse'),\n",
    "    'sfax':('sud','Sfax'),'gabes':('sud','Gabes'),'jerba':('sud','Medenine'),\n",
    "    'djerba':('sud','Medenine'),'medenine':('sud','Medenine'),'tataouine':('sud','Tataouine'),\n",
    "    'tozeur':('sud','Tozeur'),'kebili':('sud','Kebili'),'zarzis':('sud','Medenine'),\n",
    "    'gafsa':('centre','Gafsa'),'kairouan':('centre','Kairouan'),'kasserine':('centre','Kasserine'),\n",
    "    'sidi bouzid':('centre','Sidi Bouzid'),'siliana':('centre','Siliana')\n",
    "}.items()}\n",
    "\n",
    "def _is_parasite(url):\n",
    "    u=url.lower()\n",
    "    ext=os.path.splitext(u.split('?')[0])[1]\n",
    "    if ext in ('.svg','.gif','.ico','.bmp'):\n",
    "        return True\n",
    "    return any(kw in u for kw in _PARASITE_KW)\n",
    "\n",
    "def _is_photo(url):\n",
    "    u=url.lower().split('?')[0]\n",
    "    return any(u.endswith(e) for e in _PHOTO_EXT)\n",
    "\n",
    "def _find_city(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    t=_norm(text)\n",
    "    for city in sorted(CITY_GEO,key=len,reverse=True):\n",
    "        if re.search(r'\\b'+re.escape(city)+r'\\b',t):\n",
    "            zone,region=CITY_GEO[city]\n",
    "            return zone,region,city.title()\n",
    "    return None\n",
    "\n",
    "def build_location(raw,title='',desc=''):\n",
    "    for txt in [raw,title,desc[:300]]:\n",
    "        r=_find_city(txt)\n",
    "        if r:\n",
    "            zone,region,city=r\n",
    "            mun=city\n",
    "            break\n",
    "    else:\n",
    "        region='Tunisie'; mun='Tunisie'; zone='autre'\n",
    "    parts=[p.strip() for p in (raw or '').replace(' - ',',').split(',') if p.strip()]\n",
    "    if parts and _find_city(parts[0]):\n",
    "        mun=parts[0].title()\n",
    "    return {\n",
    "        'region':region,'municipality':mun,'zone':zone,\n",
    "        'location_details':{'region':region,'municipality':mun,'zone':zone,'raw':raw}\n",
    "    }\n",
    "\n",
    "def get_conn():\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def url_exists(cur,u):\n",
    "    cur.execute('SELECT 1 FROM fi_dari_listings WHERE url=%s',(u,))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "def extract_rooms(t):\n",
    "    if not t:\n",
    "        return None\n",
    "    t=t.lower()\n",
    "    for pat in [r'\\bs\\s*\\+?\\s*(\\d)\\b',r'\\b[ft]\\s*(\\d)\\b',r'(\\d)\\s*pi[ee]ces?',r'\\bstudio\\b']:\n",
    "        m=re.search(pat,t)\n",
    "        if m:\n",
    "            if 'studio' in pat:\n",
    "                return 1\n",
    "            v=int(m.group(1))\n",
    "            return v if 1<=v<=15 else None\n",
    "    return None\n",
    "\n",
    "def extract_surface(t):\n",
    "    if not t:\n",
    "        return None\n",
    "    m=re.search(r'([\\d][\\d\\s.,]*)\\s*m\\s*[2]',t.lower())\n",
    "    if m:\n",
    "        try:\n",
    "            v=float(m.group(1).replace(' ','').replace(',','.'))\n",
    "            return v if 10<=v<=50000 else None\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_price(t):\n",
    "    if not t:\n",
    "        return None\n",
    "    t=re.sub(r'(?i)\\b(tnd|dt|dinar)\\b','',str(t))\n",
    "    t=t.replace('\\xa0','').replace('\\u202f','')\n",
    "    d=re.sub(r'[^\\d.,]','',t.strip())\n",
    "    if not d:\n",
    "        return None\n",
    "    if re.match(r'^\\d{1,3}[.,]\\d{3}$',d):\n",
    "        d=d.replace('.','').replace(',','')\n",
    "    elif d.count('.')>1:\n",
    "        d=d.replace('.','')\n",
    "    d=d.replace(',','.')\n",
    "    try:\n",
    "        v=float(d)\n",
    "        return v if 500<=v<=100_000_000 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def download_images(urls,ptype,lid):\n",
    "    folder=os.path.join(MEDIA_ROOT,ptype,str(lid))\n",
    "    os.makedirs(folder,exist_ok=True)\n",
    "    if not urls:\n",
    "        return None,folder,0\n",
    "    saved=[]\n",
    "    hdrs={'User-Agent':'Mozilla/5.0'}\n",
    "    try:\n",
    "        from PIL import Image as _PI\n",
    "        import io as _io\n",
    "        _pil=True\n",
    "    except:\n",
    "        _pil=False\n",
    "    for url in dict.fromkeys(u for u in urls if u and not u.startswith('data:')):\n",
    "        if _is_parasite(url) or not _is_photo(url):\n",
    "            continue\n",
    "        uhd=re.sub(r'[_-](thumb|small|medium)','',url,flags=re.I)\n",
    "        for u in [uhd,url]:\n",
    "            try:\n",
    "                r=requests.get(u,headers=hdrs,timeout=12)\n",
    "                if r.status_code==200 and len(r.content)>=40_000:\n",
    "                    if _pil:\n",
    "                        try:\n",
    "                            img=_PI.open(_io.BytesIO(r.content))\n",
    "                            if img.size[0]<300 or img.size[1]<200:\n",
    "                                break\n",
    "                        except:\n",
    "                            pass\n",
    "                    ct=r.headers.get('Content-Type','')\n",
    "                    ext='webp' if 'webp' in ct else 'png' if 'png' in ct else 'jpg'\n",
    "                    fp=os.path.join(folder,f'image_{len(saved)+1:03d}.{ext}')\n",
    "                    with open(fp,'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    saved.append(fp)\n",
    "                    time.sleep(0.2)\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    return (saved[0] if saved else None),folder,len(saved)\n",
    "\n",
    "def insert_listing(cur,d):\n",
    "    try:\n",
    "        cur.execute(\n",
    "            'INSERT INTO fi_dari_listings(title,price,transaction_type,type,region,municipality,zone,location_details,surface,rooms,features,poi,description,url,pdf_link,image_path,images_folder,images_count,last_updated,is_new) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) ON CONFLICT(url) DO NOTHING RETURNING id',\n",
    "            (d.get('title'),d.get('price'),d.get('transaction_type'),d.get('type'),\n",
    "             d.get('region'),d.get('municipality'),d.get('zone'),json.dumps(d.get('location_details',{})),\n",
    "             d.get('surface'),d.get('rooms'),json.dumps(d.get('features',{})),json.dumps(d.get('poi',{})),\n",
    "             d.get('description',''),d.get('url'),d.get('pdf_link'),None,None,0,d.get('last_updated','Unknown'),True))\n",
    "        row=cur.fetchone()\n",
    "        return row[0] if row else None\n",
    "    except Exception as e:\n",
    "        print(f'Insert err:{e}')\n",
    "        return None\n",
    "\n",
    "def update_images(cur,lid,t,f,c):\n",
    "    cur.execute('UPDATE fi_dari_listings SET image_path=%s,images_folder=%s,images_count=%s WHERE id=%s',(t,f,c,lid))\n",
    "\n",
    "def log_start(cur,rt):\n",
    "    cur.execute('INSERT INTO fi_dari_log(run_type) VALUES(%s) RETURNING id',(rt,))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def log_end(cur,lid,n,e,s='done'):\n",
    "    cur.execute('UPDATE fi_dari_log SET finished_at=NOW(),total_new=%s,total_errors=%s,status=%s WHERE id=%s',(n,e,s,lid))\n",
    "\n",
    "def reset_is_new(cur):\n",
    "    cur.execute('UPDATE fi_dari_listings SET is_new=FALSE')\n",
    "\n",
    "def create_driver():\n",
    "    opts=Options()\n",
    "    opts.add_argument('--headless=new')\n",
    "    opts.add_argument('--no-sandbox')\n",
    "    opts.add_argument('--disable-dev-shm-usage')\n",
    "    opts.add_argument('--window-size=1920,1080')\n",
    "    opts.add_experimental_option('excludeSwitches',['enable-automation'])\n",
    "    opts.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36')\n",
    "    opts.set_capability('goog:loggingPrefs',{'performance':'ALL'})\n",
    "    d=webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=opts)\n",
    "    d.execute_script(\"Object.defineProperty(navigator,'webdriver',{get:()=>undefined})\")\n",
    "    return d\n",
    "\n",
    "def deep_scroll(driver,wait=10):\n",
    "    time.sleep(wait)\n",
    "    total=driver.execute_script('return document.body.scrollHeight')\n",
    "    step=max(200,total//15)\n",
    "    pos=0\n",
    "    while pos<total:\n",
    "        driver.execute_script(f'window.scrollTo(0,{pos});')\n",
    "        time.sleep(0.3)\n",
    "        pos+=step\n",
    "        nh=driver.execute_script('return document.body.scrollHeight')\n",
    "        if nh>total:\n",
    "            total=nh\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "    time.sleep(2)\n",
    "    driver.execute_script('window.scrollTo(0,0);')\n",
    "    time.sleep(1)\n",
    "\n",
    "def get_annonces(driver,url):\n",
    "    driver.get(url)\n",
    "    deep_scroll(driver,10)\n",
    "    soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "    urls=set()\n",
    "    for a in soup.find_all('a',href=True):\n",
    "        h=a['href']\n",
    "        if any(p in h for p in ['/bien/','/annonce/','/detail/']):\n",
    "            if not h.startswith('http'):\n",
    "                h=BASE_URL+h\n",
    "            urls.add(h)\n",
    "    return [{'url':u,'title_hint':'','price_hint':'','location_hint':'','thumb':None} for u in urls]\n",
    "\n",
    "def scrape_detail(driver,url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        deep_scroll(driver,7)\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        data={}\n",
    "        h1=soup.find('h1')\n",
    "        data['title_detail']=h1.get_text(strip=True) if h1 else ''\n",
    "        desc=''; best=0\n",
    "        for tag in soup.find_all(['div','p','section']):\n",
    "            cs=' '.join(tag.get('class',[]))\n",
    "            if any(x in cs.lower() for x in ['description','detail','content','text','body']):\n",
    "                t=tag.get_text(' ',strip=True)\n",
    "                if 30<len(t)<5000 and len(t)>best:\n",
    "                    best=len(t); desc=t\n",
    "        data['description']=desc\n",
    "        ptext=re.compile(r'([\\d][\\d\\s.,]*)[\\s]*(?:dt|tnd|dinar)',re.I)\n",
    "        m=ptext.search(soup.get_text(' '))\n",
    "        if m:\n",
    "            data['price']=extract_price(m.group(1))\n",
    "        for tag in soup.find_all('time'):\n",
    "            data['last_updated']=tag.get('datetime','') or tag.get_text(strip=True)\n",
    "            break\n",
    "        data.setdefault('last_updated','Unknown')\n",
    "        for a in soup.find_all('a',href=True):\n",
    "            h=a['href']\n",
    "            txt=a.get_text(strip=True).lower()\n",
    "            if h.lower().endswith('.pdf') or any(k in txt for k in ['fiche','telecharger','pdf']):\n",
    "                data['pdf_link']=h if h.startswith('http') else BASE_URL+'/'+h.lstrip('/')\n",
    "                break\n",
    "        data.setdefault('pdf_link',None)\n",
    "        loc=''\n",
    "        for nav in soup.find_all(['nav','ol'],class_=re.compile(r'breadcrumb|ariane',re.I)):\n",
    "            parts=[i.get_text(strip=True) for i in nav.find_all(['li','a','span']) if i.get_text(strip=True)]\n",
    "            geo=[p for p in parts[1:-1] if 2<len(p)<60]\n",
    "            if geo:\n",
    "                loc=' - '.join(geo)\n",
    "                break\n",
    "        if not loc:\n",
    "            m2=re.search(r'/bien/([^/?#]+)',url)\n",
    "            if m2:\n",
    "                loc=m2.group(1).replace('-',' ')\n",
    "        data['location_raw']=loc\n",
    "        imgs=[]; seen=set()\n",
    "        def add(src):\n",
    "            if not src or len(src)<15:\n",
    "                return\n",
    "            if src.startswith('//'):\n",
    "                src='https:'+src\n",
    "            if not src.startswith('http'):\n",
    "                src=BASE_URL+'/'+src.lstrip('/')\n",
    "            if not _is_parasite(src) and _is_photo(src) and src not in seen:\n",
    "                seen.add(src); imgs.append(src)\n",
    "        for img in soup.find_all('img'):\n",
    "            for attr in ['data-big','data-original','data-src','src']:\n",
    "                src=img.get(attr,'')\n",
    "                if src:\n",
    "                    add(src)\n",
    "                    break\n",
    "        data['img_urls']=imgs\n",
    "        data['surface_detail']=extract_surface(desc)\n",
    "        data['rooms_detail']=extract_rooms(desc)\n",
    "        result={}\n",
    "        for li in soup.find_all('li'):\n",
    "            t=li.get_text(strip=True)\n",
    "            if 2<len(t)<80:\n",
    "                for fr,en in FEATURE_MAPPING.items():\n",
    "                    if fr.lower() in t.lower():\n",
    "                        result[en]=True\n",
    "                        break\n",
    "        data['features']=result\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f'detail err:{e}')\n",
    "        return {}\n",
    "\n",
    "def build_url(slug,page):\n",
    "    base=f'{BASE_URL}/{slug}'\n",
    "    base=re.sub(r'&page=\\d+','',base)\n",
    "    return f'{base}&page={page}'\n",
    "\n",
    "def run_daily():\n",
    "    now=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sep='='*60\n",
    "    print(f'\\n{sep}\\n  CYCLE QUOTIDIEN Fi-Dari - {now}\\n{sep}')\n",
    "    os.makedirs(MEDIA_ROOT,exist_ok=True)\n",
    "    conn=get_conn(); cur=conn.cursor(); driver=create_driver()\n",
    "    log_id=log_start(cur,'daily'); reset_is_new(cur); conn.commit()\n",
    "    total_new=0; total_errors=0\n",
    "    try:\n",
    "        for slug,(tt,pt) in CATEGORIES.items():\n",
    "            cat_new=0\n",
    "            print(f'\\n  {pt} | {tt}')\n",
    "            for page_num in range(1,MAX_PAGES_DAILY+1):\n",
    "                url_page=build_url(slug,page_num)\n",
    "                anns=[]\n",
    "                try:\n",
    "                    anns=get_annonces(driver,url_page)\n",
    "                except:\n",
    "                    driver=create_driver()\n",
    "                if not anns:\n",
    "                    break\n",
    "                new_this=0\n",
    "                for ann in anns:\n",
    "                    ann_url=ann.get('url','')\n",
    "                    if not ann_url or url_exists(cur,ann_url):\n",
    "                        continue\n",
    "                    print(f'    NEW {ann_url[-60:]}')\n",
    "                    det={}\n",
    "                    try:\n",
    "                        det=scrape_detail(driver,ann_url)\n",
    "                    except:\n",
    "                        pass\n",
    "                    price=det.get('price') or extract_price(ann.get('price_hint',''))\n",
    "                    loc_raw=det.get('location_raw','') or ann.get('location_hint','')\n",
    "                    desc=det.get('description','')\n",
    "                    title=det.get('title_detail','') or 'N/A'\n",
    "                    geo=build_location(loc_raw,title,desc)\n",
    "                    cfg=PROPERTY_CONFIG.get(pt,{'has_rooms':True,'has_surface':True})\n",
    "                    surface=(det.get('surface_detail') or extract_surface(desc)) if cfg['has_surface'] else None\n",
    "                    rooms=(det.get('rooms_detail') or extract_rooms(desc)) if cfg['has_rooms'] else None\n",
    "                    data={\n",
    "                        'title':title,'price':price,'transaction_type':tt,'type':pt,\n",
    "                        **{k:geo[k] for k in ['region','municipality','zone','location_details']},\n",
    "                        'surface':surface,'rooms':rooms,'features':det.get('features',{}),'poi':{},\n",
    "                        'description':desc,'url':ann_url,'pdf_link':det.get('pdf_link'),\n",
    "                        'last_updated':det.get('last_updated','Unknown')\n",
    "                    }\n",
    "                    lid=insert_listing(cur,data)\n",
    "                    if not lid:\n",
    "                        total_errors+=1\n",
    "                        continue\n",
    "                    conn.commit()\n",
    "                    imgs=list(det.get('img_urls',[]))\n",
    "                    if ann.get('thumb') and not _is_parasite(ann['thumb']) and _is_photo(ann['thumb']):\n",
    "                        if ann['thumb'] not in imgs:\n",
    "                            imgs.insert(0,ann['thumb'])\n",
    "                    t,f,n=download_images(imgs,pt,lid)\n",
    "                    update_images(cur,lid,t,f,n)\n",
    "                    conn.commit()\n",
    "                    cat_new+=1; total_new+=1; new_this+=1\n",
    "                    mun=geo['municipality']\n",
    "                    rgn=geo['region']\n",
    "                    print(f'    OK id={lid} {mun},{rgn} prix={price} imgs={n}')\n",
    "                    time.sleep(random.uniform(2,4))\n",
    "                if new_this==0:\n",
    "                    break\n",
    "                time.sleep(random.uniform(2,5))\n",
    "            print(f'  {pt}_{tt} -> {cat_new} nouvelles')\n",
    "    except Exception as e:\n",
    "        print(f'ERREUR:{e}')\n",
    "        log_end(cur,log_id,total_new,total_errors,'error')\n",
    "        conn.commit()\n",
    "    else:\n",
    "        log_end(cur,log_id,total_new,total_errors,'done')\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    print(f'\\n  TERMINE - {total_new} nouvelles annonces')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    run_daily()\n",
    "\"\"\"\n",
    "\n",
    "with open(DAILY_PY, 'w', encoding='utf-8') as f:\n",
    "    f.write(DAILY_CONTENT)\n",
    "print(f\"OK {DAILY_PY}\")\n",
    "\n",
    "bat_content = (\n",
    "    \"@echo off\\n\"\n",
    "    \"echo ==========================================\\n\"\n",
    "    f\"echo  Fi-Dari Daily Scraper - %date% %time%\\n\"\n",
    "    \"echo ==========================================\\n\"\n",
    "    f'cd /d \"{PROJECT_DIR}\"\\n'\n",
    "    f'\"{PYTHON_EXE}\" daily_scrape.py >> \"logs\\\\daily_log.txt\" 2>&1\\n'\n",
    "    f'echo Termine : %date% %time% >> \"logs\\\\daily_log.txt\"\\n'\n",
    ")\n",
    "with open(BAT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write(bat_content)\n",
    "print(f\"OK {BAT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642c612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "═════════════════════════════════════════════════════════════════\n",
      "  📊 STATISTIQUES PAR CATÉGORIE\n",
      "═════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>total</th>\n",
       "      <th>avec_prix</th>\n",
       "      <th>prix_moyen</th>\n",
       "      <th>avec_rooms</th>\n",
       "      <th>avec_surface</th>\n",
       "      <th>surface_moy</th>\n",
       "      <th>avec_pdf</th>\n",
       "      <th>total_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apartment</td>\n",
       "      <td>rent</td>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>2282.0</td>\n",
       "      <td>200</td>\n",
       "      <td>101</td>\n",
       "      <td>101.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apartment</td>\n",
       "      <td>sale</td>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>328031.0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>455.3</td>\n",
       "      <td>0</td>\n",
       "      <td>4166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house</td>\n",
       "      <td>rent</td>\n",
       "      <td>148</td>\n",
       "      <td>120</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>128</td>\n",
       "      <td>108</td>\n",
       "      <td>170.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>house</td>\n",
       "      <td>sale</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>333258.0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>land</td>\n",
       "      <td>rent</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>279550.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>land</td>\n",
       "      <td>sale</td>\n",
       "      <td>201</td>\n",
       "      <td>199</td>\n",
       "      <td>626736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>1064.3</td>\n",
       "      <td>2</td>\n",
       "      <td>3004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>200</td>\n",
       "      <td>194</td>\n",
       "      <td>117218.0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>469.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>office</td>\n",
       "      <td>sale</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>2700691.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>872.0</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type transaction_type  total  avec_prix  prix_moyen  avec_rooms  \\\n",
       "0  apartment             rent    200        198      2282.0         200   \n",
       "1  apartment             sale    200        190    328031.0         200   \n",
       "2      house             rent    148        120      2730.0         128   \n",
       "3      house             sale    200        199    333258.0         200   \n",
       "4       land             rent      5          4    279550.0           0   \n",
       "5       land             sale    201        199    626736.0           0   \n",
       "6     office             rent    200        194    117218.0           0   \n",
       "7     office             sale     24         17   2700691.0           0   \n",
       "\n",
       "   avec_surface  surface_moy  avec_pdf  total_images  \n",
       "0           101        101.4         0          2925  \n",
       "1           200        455.3         0          4166  \n",
       "2           108        170.9         0          1459  \n",
       "3           200        193.0         0          4046  \n",
       "4             4       2520.0         0            52  \n",
       "5           198       1064.3         2          3004  \n",
       "6           192        469.5         2          1744  \n",
       "7            24        872.0         0           328  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  🆕 20 DERNIÈRES ANNONCES\n",
      "═════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>rooms</th>\n",
       "      <th>surface</th>\n",
       "      <th>price</th>\n",
       "      <th>region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>zone</th>\n",
       "      <th>images_count</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178</td>\n",
       "      <td>Achat terrain  de 145m²</td>\n",
       "      <td>land</td>\n",
       "      <td>sale</td>\n",
       "      <td>None</td>\n",
       "      <td>145.0</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>Bizerte</td>\n",
       "      <td>Bizerte Pa32070</td>\n",
       "      <td>nord</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2026-02-21 22:50:28.040989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1177</td>\n",
       "      <td>Location bureau de 135m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>Soukra</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>9</td>\n",
       "      <td>9 septembre 2024</td>\n",
       "      <td>2026-02-21 21:17:13.662477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1176</td>\n",
       "      <td>Location bureau de 250m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>250.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>8</td>\n",
       "      <td>11 septembre 2024</td>\n",
       "      <td>2026-02-21 21:16:50.185808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>Location bureau de 114m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>4</td>\n",
       "      <td>12 septembre 2024</td>\n",
       "      <td>2026-02-21 21:16:29.608177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1174</td>\n",
       "      <td>Location bureau de 101m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>4</td>\n",
       "      <td>12 septembre 2024</td>\n",
       "      <td>2026-02-21 21:16:08.094144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1173</td>\n",
       "      <td>Location bureau de 107m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Centre Urbain Nord</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>5</td>\n",
       "      <td>10 septembre 2024</td>\n",
       "      <td>2026-02-21 21:15:46.879898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1172</td>\n",
       "      <td>Location bureau de 70m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>6</td>\n",
       "      <td>4 septembre 2024</td>\n",
       "      <td>2026-02-21 21:15:25.080956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1171</td>\n",
       "      <td>Location bureau de 75m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>4</td>\n",
       "      <td>11 octobre 2024</td>\n",
       "      <td>2026-02-21 21:15:04.391276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1170</td>\n",
       "      <td>Location bureau de 350m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>350.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>4</td>\n",
       "      <td>10 septembre 2024</td>\n",
       "      <td>2026-02-21 21:14:44.410577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1169</td>\n",
       "      <td>Location bureau de 65m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>3</td>\n",
       "      <td>13 septembre 2024</td>\n",
       "      <td>2026-02-21 21:14:23.979872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1168</td>\n",
       "      <td>Location bureau de 492m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>492.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>3</td>\n",
       "      <td>6 septembre 2024</td>\n",
       "      <td>2026-02-21 21:14:03.397774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167</td>\n",
       "      <td>Location bureau de 80m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>80.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>8</td>\n",
       "      <td>7 octobre 2024</td>\n",
       "      <td>2026-02-21 21:13:14.977244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1166</td>\n",
       "      <td>Location bureau de 50m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Ain Zaghouan Nord</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>6</td>\n",
       "      <td>5 octobre 2024</td>\n",
       "      <td>2026-02-21 21:12:53.840488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1165</td>\n",
       "      <td>Location bureau de 258m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>258.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>6</td>\n",
       "      <td>4 octobre 2024</td>\n",
       "      <td>2026-02-21 21:12:31.627221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1164</td>\n",
       "      <td>Location bureau de 160m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>La Marsa</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>5</td>\n",
       "      <td>4 octobre 2024</td>\n",
       "      <td>2026-02-21 21:12:11.147213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1163</td>\n",
       "      <td>Location bureau de 200m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Ain Zaghouan</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>6</td>\n",
       "      <td>17 septembre 2024</td>\n",
       "      <td>2026-02-21 21:11:50.116590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1162</td>\n",
       "      <td>Location bureau de 160m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>La Marsa</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>6</td>\n",
       "      <td>5 octobre 2024</td>\n",
       "      <td>2026-02-21 21:11:27.987473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1161</td>\n",
       "      <td>Location bureau de 90m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>2</td>\n",
       "      <td>11 octobre 2024</td>\n",
       "      <td>2026-02-21 21:11:10.065256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1160</td>\n",
       "      <td>Location bureau de 90m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>9</td>\n",
       "      <td>23 septembre 2024</td>\n",
       "      <td>2026-02-21 21:10:48.249696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1159</td>\n",
       "      <td>Location bureau de 270m²</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>None</td>\n",
       "      <td>270.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>7</td>\n",
       "      <td>1 octobre 2024</td>\n",
       "      <td>2026-02-21 21:10:27.355019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                     title    type transaction_type rooms  surface  \\\n",
       "0   1178   Achat terrain  de 145m²    land             sale  None    145.0   \n",
       "1   1177  Location bureau de 135m²  office             rent  None    135.0   \n",
       "2   1176  Location bureau de 250m²  office             rent  None    250.0   \n",
       "3   1175  Location bureau de 114m²  office             rent  None    114.0   \n",
       "4   1174  Location bureau de 101m²  office             rent  None    101.0   \n",
       "5   1173  Location bureau de 107m²  office             rent  None    107.0   \n",
       "6   1172   Location bureau de 70m²  office             rent  None     70.0   \n",
       "7   1171   Location bureau de 75m²  office             rent  None     75.0   \n",
       "8   1170  Location bureau de 350m²  office             rent  None    350.0   \n",
       "9   1169   Location bureau de 65m²  office             rent  None     65.0   \n",
       "10  1168  Location bureau de 492m²  office             rent  None    492.0   \n",
       "11  1167   Location bureau de 80m²  office             rent  None     80.0   \n",
       "12  1166   Location bureau de 50m²  office             rent  None     50.0   \n",
       "13  1165  Location bureau de 258m²  office             rent  None    258.0   \n",
       "14  1164  Location bureau de 160m²  office             rent  None    160.0   \n",
       "15  1163  Location bureau de 200m²  office             rent  None    200.0   \n",
       "16  1162  Location bureau de 160m²  office             rent  None    160.0   \n",
       "17  1161   Location bureau de 90m²  office             rent  None     90.0   \n",
       "18  1160   Location bureau de 90m²  office             rent  None     90.0   \n",
       "19  1159  Location bureau de 270m²  office             rent  None    270.0   \n",
       "\n",
       "       price   region        municipality         zone  images_count  \\\n",
       "0   170000.0  Bizerte     Bizerte Pa32070         nord             0   \n",
       "1        NaN   Ariana              Soukra  grand_tunis             9   \n",
       "2     8000.0    Tunis               Tunis  grand_tunis             8   \n",
       "3     2300.0    Tunis               Tunis  grand_tunis             4   \n",
       "4     2100.0    Tunis               Tunis  grand_tunis             4   \n",
       "5     2200.0    Tunis  Centre Urbain Nord  grand_tunis             5   \n",
       "6     1900.0    Tunis               Tunis  grand_tunis             6   \n",
       "7     1700.0    Tunis               Tunis  grand_tunis             4   \n",
       "8     8500.0    Tunis               Tunis  grand_tunis             4   \n",
       "9     1950.0    Tunis               Tunis  grand_tunis             3   \n",
       "10       NaN    Tunis               Tunis  grand_tunis             3   \n",
       "11     800.0    Tunis               Tunis  grand_tunis             8   \n",
       "12    1300.0    Tunis   Ain Zaghouan Nord  grand_tunis             6   \n",
       "13    4500.0    Tunis               Tunis  grand_tunis             6   \n",
       "14    2200.0    Tunis            La Marsa  grand_tunis             5   \n",
       "15    3800.0    Tunis        Ain Zaghouan  grand_tunis             6   \n",
       "16    2200.0    Tunis            La Marsa  grand_tunis             6   \n",
       "17       NaN    Tunis               Tunis  grand_tunis             2   \n",
       "18    1400.0    Tunis               Tunis  grand_tunis             9   \n",
       "19    6400.0    Tunis               Tunis  grand_tunis             7   \n",
       "\n",
       "         last_updated                 scraped_at  \n",
       "0             Unknown 2026-02-21 22:50:28.040989  \n",
       "1    9 septembre 2024 2026-02-21 21:17:13.662477  \n",
       "2   11 septembre 2024 2026-02-21 21:16:50.185808  \n",
       "3   12 septembre 2024 2026-02-21 21:16:29.608177  \n",
       "4   12 septembre 2024 2026-02-21 21:16:08.094144  \n",
       "5   10 septembre 2024 2026-02-21 21:15:46.879898  \n",
       "6    4 septembre 2024 2026-02-21 21:15:25.080956  \n",
       "7     11 octobre 2024 2026-02-21 21:15:04.391276  \n",
       "8   10 septembre 2024 2026-02-21 21:14:44.410577  \n",
       "9   13 septembre 2024 2026-02-21 21:14:23.979872  \n",
       "10   6 septembre 2024 2026-02-21 21:14:03.397774  \n",
       "11     7 octobre 2024 2026-02-21 21:13:14.977244  \n",
       "12     5 octobre 2024 2026-02-21 21:12:53.840488  \n",
       "13     4 octobre 2024 2026-02-21 21:12:31.627221  \n",
       "14     4 octobre 2024 2026-02-21 21:12:11.147213  \n",
       "15  17 septembre 2024 2026-02-21 21:11:50.116590  \n",
       "16     5 octobre 2024 2026-02-21 21:11:27.987473  \n",
       "17    11 octobre 2024 2026-02-21 21:11:10.065256  \n",
       "18  23 septembre 2024 2026-02-21 21:10:48.249696  \n",
       "19     1 octobre 2024 2026-02-21 21:10:27.355019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  🗺️  RÉPARTITION GÉOGRAPHIQUE\n",
      "═════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone</th>\n",
       "      <th>region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>La Marsa</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Hammamet</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>Soukra</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Beni Khiar</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Carthage</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Hammamet Nord</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>El Menzah</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>Mrezga</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Ain Zaghouan</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ben Arous</td>\n",
       "      <td>Ben Arous</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Ain Zaghouan Nord</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>El Aouina</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Nabeul Centre</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sud</td>\n",
       "      <td>Médenine</td>\n",
       "      <td>Djerba</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ariana</td>\n",
       "      <td>Ennasr</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>grand_tunis</td>\n",
       "      <td>Ben Arous</td>\n",
       "      <td>Mourouj</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nord</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>Hammamet Sud</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           zone     region       municipality  total\n",
       "0          nord     Nabeul             Nabeul    333\n",
       "1   grand_tunis      Tunis              Tunis    170\n",
       "2   grand_tunis      Tunis           La Marsa     84\n",
       "3          nord     Nabeul           Hammamet     65\n",
       "4   grand_tunis     Ariana             Soukra     50\n",
       "5          nord     Nabeul         Beni Khiar     49\n",
       "6   grand_tunis      Tunis           Carthage     49\n",
       "7          nord     Nabeul      Hammamet Nord     43\n",
       "8   grand_tunis     Ariana          El Menzah     37\n",
       "9   grand_tunis     Ariana             Mrezga     30\n",
       "10  grand_tunis      Tunis       Ain Zaghouan     30\n",
       "11  grand_tunis  Ben Arous          Ben Arous     23\n",
       "12  grand_tunis     Ariana             Ariana     19\n",
       "13  grand_tunis      Tunis  Ain Zaghouan Nord     17\n",
       "14  grand_tunis      Tunis          El Aouina     16\n",
       "15         nord     Nabeul      Nabeul Centre     15\n",
       "16          sud   Médenine             Djerba     13\n",
       "17  grand_tunis     Ariana             Ennasr     13\n",
       "18  grand_tunis  Ben Arous            Mourouj     12\n",
       "19         nord     Nabeul       Hammamet Sud     11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  🖼️  TOP ANNONCES PAR IMAGES\n",
      "═════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>images_count</th>\n",
       "      <th>images_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>587</td>\n",
       "      <td>house</td>\n",
       "      <td>Achat maison S+6 de 280m²</td>\n",
       "      <td>48</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>apartment</td>\n",
       "      <td>Location appartement s+2 de 80m²</td>\n",
       "      <td>44</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "      <td>house</td>\n",
       "      <td>Location maison s+4 de 200m²</td>\n",
       "      <td>44</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610</td>\n",
       "      <td>house</td>\n",
       "      <td>Location maison s+3 de 100m²</td>\n",
       "      <td>42</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>house</td>\n",
       "      <td>Location maison s+3 de 370m²</td>\n",
       "      <td>42</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1042</td>\n",
       "      <td>office</td>\n",
       "      <td>Location bureau de 250m²</td>\n",
       "      <td>41</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81</td>\n",
       "      <td>apartment</td>\n",
       "      <td>Achat appartement s+3 de 159m²</td>\n",
       "      <td>41</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>227</td>\n",
       "      <td>apartment</td>\n",
       "      <td>Location appartement s+3 de 200m²</td>\n",
       "      <td>40</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>435</td>\n",
       "      <td>house</td>\n",
       "      <td>Achat maison s+2 de 180m²</td>\n",
       "      <td>39</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>217</td>\n",
       "      <td>apartment</td>\n",
       "      <td>Location appartement s+3 de 130m²</td>\n",
       "      <td>38</td>\n",
       "      <td>C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id       type                              title  images_count  \\\n",
       "0   587      house          Achat maison S+6 de 280m²            48   \n",
       "1   386  apartment   Location appartement s+2 de 80m²            44   \n",
       "2   648      house       Location maison s+4 de 200m²            44   \n",
       "3   610      house       Location maison s+3 de 100m²            42   \n",
       "4   604      house       Location maison s+3 de 370m²            42   \n",
       "5  1042     office           Location bureau de 250m²            41   \n",
       "6    81  apartment     Achat appartement s+3 de 159m²            41   \n",
       "7   227  apartment  Location appartement s+3 de 200m²            40   \n",
       "8   435      house          Achat maison s+2 de 180m²            39   \n",
       "9   217  apartment  Location appartement s+3 de 130m²            38   \n",
       "\n",
       "                                       images_folder  \n",
       "0  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "1  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "2  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "3  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "4  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "5  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "6  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "7  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "8  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  \n",
       "9  C:\\Users\\chtou\\Desktop\\scrapping\\media_fidari\\...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  📋 HISTORIQUE CYCLES\n",
      "═════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>finished_at</th>\n",
       "      <th>total_new</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily</td>\n",
       "      <td>2026-02-21 23:06:42.891916</td>\n",
       "      <td>2026-02-21 23:07:02.910761</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily</td>\n",
       "      <td>2026-02-21 22:54:29.812294</td>\n",
       "      <td>2026-02-21 22:54:49.975220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily</td>\n",
       "      <td>2026-02-21 22:49:47.348499</td>\n",
       "      <td>2026-02-21 22:52:14.797525</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>initial</td>\n",
       "      <td>2026-02-21 11:41:13.644112</td>\n",
       "      <td>2026-02-21 21:17:40.656249</td>\n",
       "      <td>1177</td>\n",
       "      <td>0</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  run_type                 started_at                finished_at  total_new  \\\n",
       "0    daily 2026-02-21 23:06:42.891916 2026-02-21 23:07:02.910761          0   \n",
       "1    daily 2026-02-21 22:54:29.812294 2026-02-21 22:54:49.975220          0   \n",
       "2    daily 2026-02-21 22:49:47.348499 2026-02-21 22:52:14.797525          1   \n",
       "3  initial 2026-02-21 11:41:13.644112 2026-02-21 21:17:40.656249       1177   \n",
       "\n",
       "   total_errors status  \n",
       "0             0   done  \n",
       "1             0   done  \n",
       "2             0   done  \n",
       "3             0   done  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# CELLULE 9 — STATISTIQUES\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "print(\"═\"*65); print(\"  📊 STATISTIQUES PAR CATÉGORIE\"); print(\"═\"*65)\n",
    "pd.read_sql(\"\"\"\n",
    "    SELECT type, transaction_type,\n",
    "           COUNT(*)                       AS total,\n",
    "           COUNT(price)                   AS avec_prix,\n",
    "           ROUND(AVG(price)::numeric, 0)  AS prix_moyen,\n",
    "           COUNT(rooms)                   AS avec_rooms,\n",
    "           COUNT(surface)                 AS avec_surface,\n",
    "           ROUND(AVG(surface)::numeric,1) AS surface_moy,\n",
    "           COUNT(pdf_link)                AS avec_pdf,\n",
    "           SUM(images_count)              AS total_images\n",
    "    FROM fi_dari_listings\n",
    "    GROUP BY type, transaction_type\n",
    "    ORDER BY type, transaction_type\n",
    "\"\"\", conn).pipe(display)\n",
    "\n",
    "print(\"\\n\"+\"═\"*65); print(\"  🆕 20 DERNIÈRES ANNONCES\"); print(\"═\"*65)\n",
    "pd.read_sql(\"\"\"\n",
    "    SELECT id, title, type, transaction_type,\n",
    "           rooms, surface, price,\n",
    "           region, municipality, zone,\n",
    "           images_count, last_updated, scraped_at\n",
    "    FROM fi_dari_listings\n",
    "    ORDER BY scraped_at DESC LIMIT 20\n",
    "\"\"\", conn).pipe(display)\n",
    "\n",
    "print(\"\\n\"+\"═\"*65); print(\"  🗺️  RÉPARTITION GÉOGRAPHIQUE\"); print(\"═\"*65)\n",
    "pd.read_sql(\"\"\"\n",
    "    SELECT zone, region, municipality,\n",
    "           COUNT(*) AS total\n",
    "    FROM fi_dari_listings\n",
    "    GROUP BY zone, region, municipality\n",
    "    ORDER BY total DESC LIMIT 20\n",
    "\"\"\", conn).pipe(display)\n",
    "\n",
    "print(\"\\n\"+\"═\"*65); print(\"  🖼️  TOP ANNONCES PAR IMAGES\"); print(\"═\"*65)\n",
    "pd.read_sql(\"\"\"\n",
    "    SELECT id, type, title, images_count, images_folder\n",
    "    FROM fi_dari_listings\n",
    "    WHERE images_count > 0\n",
    "    ORDER BY images_count DESC LIMIT 10\n",
    "\"\"\", conn).pipe(display)\n",
    "\n",
    "print(\"\\n\"+\"═\"*65); print(\"  📋 HISTORIQUE CYCLES\"); print(\"═\"*65)\n",
    "pd.read_sql(\"\"\"\n",
    "    SELECT run_type, started_at, finished_at,\n",
    "           total_new, total_errors, status\n",
    "    FROM fi_dari_log\n",
    "    ORDER BY started_at DESC LIMIT 10\n",
    "\"\"\", conn).pipe(display)\n",
    "\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
